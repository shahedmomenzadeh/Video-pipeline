{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9Iy2psGagD6"
      },
      "source": [
        "# Pre training video dataset creating from YouTube videos\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxRJdEmaafkh",
        "outputId": "b8ae07ed-e99d-42f7-c7b0-60e55a844411"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/pre_training_phase_data\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/pre_training_phase_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffU-Juy2qcp-",
        "outputId": "7aa212d8-8024-43f9-be7c-98f232e6ef40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "audio  transcripts  videos  www.youtube.com_cookies.txt\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLibrFKFbx4N",
        "outputId": "848fcdb7-8753-43c2-9cc2-2aa5b367cf93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Videos 250\n",
            "Number of Audios 246\n",
            "Number of transcripts 181\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "videos_list = os.listdir(\"./videos\")\n",
        "print(f\"Number of Videos {len(videos_list)}\")\n",
        "print(f\"Number of Audios {len(os.listdir('./audio'))}\")\n",
        "print(f\"Number of transcripts {len(os.listdir('./transcripts'))}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3awSaZHXZmiN"
      },
      "source": [
        "# Download YouTube Videos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ii0I2iJdZ2Of",
        "outputId": "f13d56f9-e408-4be3-b183-2175b7e65b38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/176.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m176.0/176.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/3.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m140.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q yt-dlp pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiKw1th1HfUY"
      },
      "outputs": [],
      "source": [
        "import yt_dlp\n",
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import shutil\n",
        "import subprocess\n",
        "import time  # Import the time module\n",
        "\n",
        "\n",
        "def is_ffmpeg_installed():\n",
        "    \"\"\"Check if FFmpeg is installed and available in the system's PATH.\"\"\"\n",
        "    return shutil.which(\"ffmpeg\") is not None\n",
        "\n",
        "\n",
        "def extract_audio_ffmpeg(video_filepath: str, audio_dir: str) -> str | None:\n",
        "    \"\"\"\n",
        "    Extracts audio from a video file using FFmpeg, converting it to 16kHz mono WAV.\n",
        "\n",
        "    Args:\n",
        "        video_filepath: The full path to the input video file.\n",
        "        audio_dir: The directory where the extracted audio will be saved.\n",
        "\n",
        "    Returns:\n",
        "        The filename of the extracted audio file if successful, otherwise None.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(video_filepath):\n",
        "        print(f\"‚ùå Error: Video file not found at {video_filepath}\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        video_basename = os.path.basename(video_filepath)\n",
        "        video_name_no_ext = os.path.splitext(video_basename)[0]\n",
        "        audio_filename = f\"{video_name_no_ext}.wav\"\n",
        "        output_audio_path = os.path.join(audio_dir, audio_filename)\n",
        "\n",
        "        print(f\"üéµ Extracting audio from '{video_basename}'...\")\n",
        "\n",
        "        # Command to extract audio, convert to PCM 16-bit little-endian,\n",
        "        # set sample rate to 16kHz, mono channel, and overwrite output\n",
        "        command = [\n",
        "            'ffmpeg', '-i', video_filepath, '-vn', '-acodec', 'pcm_s16le',\n",
        "            '-ar', '16000', '-ac', '1', '-y', output_audio_path\n",
        "        ]\n",
        "\n",
        "        # Run ffmpeg, suppressing stdout and stderr to keep the log clean\n",
        "        subprocess.run(command, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "        print(f\"‚úÖ Audio extracted: {output_audio_path}\")\n",
        "        return audio_filename\n",
        "\n",
        "    except subprocess.CalledProcessError:\n",
        "        print(f\"‚ùå FFmpeg error during audio extraction for {video_filepath}.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Unexpected error during audio extraction: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def download_video_and_extract_audio(video_url: str,\n",
        "                                     output_dir: str = './videos',\n",
        "                                     audio_dir: str = './audio',\n",
        "                                     metadata_file: str = './videos/video_metadata.csv',\n",
        "                                     cookie_file: str | None = None):\n",
        "    \"\"\"\n",
        "    Downloads a YouTube video, extracts its audio, logs metadata, and skips processed videos.\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    os.makedirs(audio_dir, exist_ok=True)\n",
        "\n",
        "    # Define the metadata columns\n",
        "    metadata_columns = [\n",
        "        'title', 'channel_name', 'url', 'filename',\n",
        "        'download_date', 'duration_seconds', 'resolution', 'audio_filename'\n",
        "    ]\n",
        "\n",
        "    # Load or initialize metadata DataFrame\n",
        "    if os.path.exists(metadata_file):\n",
        "        try:\n",
        "            metadata_df = pd.read_csv(metadata_file)\n",
        "            # Ensure all required columns exist (for backward compatibility)\n",
        "            for col in metadata_columns:\n",
        "                if col not in metadata_df.columns:\n",
        "                    metadata_df[col] = None\n",
        "            # Reorder columns for consistency\n",
        "            metadata_df = metadata_df[metadata_columns]\n",
        "        except pd.errors.EmptyDataError:\n",
        "            metadata_df = pd.DataFrame(columns=metadata_columns)\n",
        "    else:\n",
        "        metadata_df = pd.DataFrame(columns=metadata_columns)\n",
        "\n",
        "    # Skip if video URL already processed\n",
        "    if video_url in metadata_df['url'].values:\n",
        "        print(f\"‚è© Video already in metadata (skipped): {video_url}\")\n",
        "        return\n",
        "\n",
        "    # yt-dlp options\n",
        "    ydl_opts = {\n",
        "        # Get 480p video + best audio, merge into mp4\n",
        "        'format': 'bestvideo[height=480][ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best',\n",
        "        'outtmpl': os.path.join(output_dir, '%(title)s.%(ext)s'),\n",
        "        'noplaylist': True,\n",
        "        'merge_output_format': 'mp4',\n",
        "        'postprocessors': [{'key': 'FFmpegMetadata', 'add_chapters': False}],\n",
        "        'retries': 5,\n",
        "        'fragment_retries': 5,\n",
        "        'no_warnings': True, # Suppress warnings (like SABR)\n",
        "    }\n",
        "\n",
        "    if cookie_file:\n",
        "        ydl_opts['cookiefile'] = cookie_file\n",
        "\n",
        "    try:\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            # Extract info first without downloading\n",
        "            info = ydl.extract_info(video_url, download=False)\n",
        "            video_title = str(info.get('title', 'Unknown Title'))\n",
        "            channel_name = info.get('uploader', 'Unknown Channel')\n",
        "            duration = info.get('duration')\n",
        "            width, height = info.get('width'), info.get('height')\n",
        "            resolution = f\"{width}x{height}\" if width and height else \"N/A\"\n",
        "\n",
        "            # Get the expected downloaded video path\n",
        "            expected_video_path = ydl.prepare_filename(info)\n",
        "            video_name_no_ext = os.path.splitext(os.path.basename(expected_video_path))[0]\n",
        "            expected_audio_filename = f\"{video_name_no_ext}.wav\"\n",
        "            expected_audio_path = os.path.join(audio_dir, expected_audio_filename)\n",
        "\n",
        "            # Skip if audio file already exists\n",
        "            if os.path.exists(expected_audio_path):\n",
        "                print(f\"‚è© Audio already exists, assuming processed: {expected_audio_filename}\")\n",
        "                # Log metadata if it was missing (e.g., script interrupted)\n",
        "                if video_url not in metadata_df['url'].values:\n",
        "                    new_entry = pd.DataFrame([{\n",
        "                        'title': video_title,\n",
        "                        'channel_name': channel_name,\n",
        "                        'url': video_url,\n",
        "                        'filename': os.path.basename(expected_video_path),\n",
        "                        'download_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                        'duration_seconds': duration,\n",
        "                        'resolution': resolution,\n",
        "                        'audio_filename': expected_audio_filename\n",
        "                    }])\n",
        "                    metadata_df = pd.concat([metadata_df, new_entry], ignore_index=True)\n",
        "                    metadata_df.to_csv(metadata_file, index=False)\n",
        "                return\n",
        "\n",
        "            print(f\"‚¨áÔ∏è Downloading: '{video_title}' from channel: {channel_name}\")\n",
        "            ydl.download([video_url])\n",
        "\n",
        "            # Verify download and extract audio\n",
        "            if os.path.exists(expected_video_path):\n",
        "                print(f\"‚úÖ Download complete: {os.path.basename(expected_video_path)}\")\n",
        "                audio_filename = extract_audio_ffmpeg(expected_video_path, audio_dir)\n",
        "\n",
        "                # Log new entry to metadata\n",
        "                new_entry = pd.DataFrame([{\n",
        "                    'title': video_title,\n",
        "                    'channel_name': channel_name,\n",
        "                    'url': video_url,\n",
        "                    'filename': os.path.basename(expected_video_path),\n",
        "                    'download_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                    'duration_seconds': duration,\n",
        "                    'resolution': resolution,\n",
        "                    'audio_filename': audio_filename if audio_filename else \"N/A\"\n",
        "                }])\n",
        "                metadata_df = pd.concat([metadata_df, new_entry], ignore_index=True)\n",
        "                metadata_df.to_csv(metadata_file, index=False)\n",
        "            else:\n",
        "                print(f\"‚ùå Download reported success but file not found at '{expected_video_path}'\")\n",
        "\n",
        "    except yt_dlp.utils.DownloadError as e:\n",
        "        print(f\"‚ùå yt-dlp Download Error for {video_url}: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Unexpected error for {video_url}: {e}\")\n",
        "\n",
        "\n",
        "def verify_and_process_existing_videos(videos_dir: str, audio_dir: str):\n",
        "    \"\"\"\n",
        "    Scans the videos directory and extracts audio for any video missing its corresponding .wav file.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Verifying Existing Videos ---\")\n",
        "    if not os.path.isdir(videos_dir):\n",
        "        print(f\"‚ùå Verification skipped: '{videos_dir}' not found\")\n",
        "        return\n",
        "\n",
        "    # Get a set of audio filenames (without extension)\n",
        "    existing_audio_names = {os.path.splitext(f)[0] for f in os.listdir(audio_dir) if f.endswith('.wav')}\n",
        "    video_files = [f for f in os.listdir(videos_dir) if f.endswith(('.mp4', '.mkv', '.webm', '.mov'))]\n",
        "\n",
        "    # Find videos where the filename (without extension) is not in the audio set\n",
        "    missing_audio_videos = [\n",
        "        os.path.join(videos_dir, vf)\n",
        "        for vf in video_files\n",
        "        if os.path.splitext(vf)[0] not in existing_audio_names\n",
        "    ]\n",
        "\n",
        "    if not missing_audio_videos:\n",
        "        print(\"‚úÖ All videos have corresponding audio files.\")\n",
        "        return\n",
        "\n",
        "    print(f\"‚ö†Ô∏è Found {len(missing_audio_videos)} video(s) missing audio:\")\n",
        "    for v in missing_audio_videos:\n",
        "        print(f\"  - {os.path.basename(v)}\")\n",
        "\n",
        "    success, fail = 0, 0\n",
        "    for vpath in missing_audio_videos:\n",
        "        if extract_audio_ffmpeg(vpath, audio_dir):\n",
        "            success += 1\n",
        "        else:\n",
        "            fail += 1\n",
        "\n",
        "    print(\"\\n--- Verification Summary ---\")\n",
        "    print(f\"‚úÖ Extracted: {success}\")\n",
        "    print(f\"‚ùå Failed: {fail}\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Check for FFmpeg installation\n",
        "    if not is_ffmpeg_installed():\n",
        "        print(\"=\" * 60)\n",
        "        print(\"‚ö†Ô∏è FFmpeg is not installed or not in your system PATH.\")\n",
        "        print(\"   This script requires FFmpeg to extract audio.\")\n",
        "        print(\"   Download from: https://ffmpeg.org/download.html\")\n",
        "        print(\"=\" * 60)\n",
        "    else:\n",
        "        print(\"‚úÖ FFmpeg found.\")\n",
        "\n",
        "    # Define directories and files\n",
        "    VIDEOS_DIRECTORY = './videos'\n",
        "    AUDIO_DIRECTORY = './audio'\n",
        "    METADATA_FILE = os.path.join(VIDEOS_DIRECTORY, 'video_metadata.csv')\n",
        "\n",
        "    # List of videos to download\n",
        "    video_urls = [\n",
        "    # \"https://www.youtube.com/watch?v=OZ5SmpNFlU8&pp=ygUZY2F0YXJhY3Qgc3VyZ2VyeSBuYXJyYXRlZA%3D%3D\",\n",
        "    # \"https://youtube.com/playlist?list=PLjgfi4kp5BU7Lxk_O9GLCFzjXITHVonC0&si=ITWCQq8LzXfFRRKv\",\n",
        "    # \"https://youtube.com/playlist?list=PL88gdqtVPZep0oIpxOP1AStJPK0-Q2hEj&si=-NwuoIQ1VDi5M-Al\",\n",
        "    # \"https://youtube.com/playlist?list=PL88gdqtVPZeq0B1DAszU4uUmXbsksZsUf&si=vuURrNS4U10eE3hX\",\n",
        "    # \"https://youtube.com/playlist?list=PL88gdqtVPZepBwG8e9A4HMb-cy0DJmwvp&si=B6XLDmBGXNpYZ7GX\",\n",
        "    # \"https://www.youtube.com/playlist?list=PL88gdqtVPZerlilEVmKR-s3RTADHniLqu\",\n",
        "    # \"https://youtube.com/playlist?list=PLker0kXqgiOUV92h5wszstCLFQ-JFKATf&si=2OunF06tse7abgHt\",\n",
        "     \"https://youtube.com/playlist?list=PLker0kXqgiOVD5NNEES1UAYG7e-T6phV7&si=BQDJzsn_PHp_xFuK\",\n",
        "     \"https://youtu.be/-Q4uQ6rEExs?si=4mL-Ysen96rifmsb\",\n",
        "    \"https://youtu.be/LDhPCHvGxeA?si=xtkqrHo5gRzvUe4p\",\n",
        "    \"https://youtu.be/XjZ5r8GZq5Y?si=sr9ay6WbkXjx-RH2\",\n",
        "    \"https://youtu.be/pVTa7UhsyNc?si=2QFVOC-iNL_cSLRF\",\n",
        "    \"https://www.youtube.com/watch?v=PLSKmeAV43M\",\n",
        "    \"https://youtu.be/soJWJtHoplc?si=HROqo7yRTnzffEka\",\n",
        "    \"https://www.youtube.com/watch?v=PLSKmeAV43M\",\n",
        "    \"https://www.youtube.com/watch?v=QbeI72QmFAU&pp=ygUZY2F0YXJhY3Qgc3VyZ2VyeSBuYXJyYXRlZA%3D%3D\",\n",
        "    \"https://www.youtube.com/watch?v=6aIOKqBA-64\",\n",
        "    \"https://www.youtube.com/watch?v=yaAcqYn-Teo&pp=ygUZY2F0YXJhY3Qgc3VyZ2VyeSBuYXJyYXRlZA%3D%3D\",\n",
        "    \"https://www.youtube.com/watch?v=O60ZbtRcjik&pp=ygUZY2F0YXJhY3Qgc3VyZ2VyeSBuYXJyYXRlZA%3D%3D\",\n",
        "    \"https://www.youtube.com/watch?v=OZ5SmpNFlU8&pp=ygUZY2F0YXJhY3Qgc3VyZ2VyeSBuYXJyYXRlZA%3D%3D\",\n",
        "    \"https://www.youtube.com/watch?v=n_3cG9oeuNo&pp=ygUZY2F0YXJhY3Qgc3VyZ2VyeSBuYXJyYXRlZA%3D%3D\",\n",
        "    \"https://www.youtube.com/watch?v=-Q4uQ6rEExs&pp=ygUZY2F0YXJhY3Qgc3VyZ2VyeSBuYXJyYXRlZA%3D%3D\",\n",
        "    \"https://www.youtube.com/watch?v=moF1tUd9Flc&pp=ygUZY2F0YXJhY3Qgc3VyZ2VyeSBuYXJyYXRlZA%3D%3D\",\n",
        "    \"https://www.youtube.com/watch?v=aohAHNYpAOs&pp=ygUZY2F0YXJhY3Qgc3VyZ2VyeSBuYXJyYXRlZA%3D%3D\",\n",
        "    \"https://www.youtube.com/watch?v=SrzOrek2PVg\",\n",
        "    \"https://www.youtube.com/watch?v=wtw7m6C9HAw\",\n",
        "    \"https://www.youtube.com/watch?v=mfQXeuUcJdU&pp=0gcJCfsJAYcqIYzv\",\n",
        "    \"https://www.youtube.com/watch?v=s9CYeGi7ecs\",\n",
        "    \"https://www.youtube.com/watch?v=HPX8EBCVm_s\",\n",
        "    \"https://www.youtube.com/watch?v=vWCCDQgK06U&pp=0gcJCfsJAYcqIYzv\",\n",
        "    \"https://www.youtube.com/watch?v=2qEe2REdhWw\",\n",
        "    \"https://www.youtube.com/watch?v=3wwr5EzC0r4&pp=0gcJCfsJAYcqIYzv\",\n",
        "    \"https://www.youtube.com/watch?v=Q4ez4-t3WhE\",\n",
        "    \"https://www.youtube.com/watch?v=vWCCDQgK06U\",\n",
        "    \"https://www.youtube.com/watch?v=AV2ZRjYKpSA\",\n",
        "    \"https://www.youtube.com/watch?v=vfmohnIFMOQ\",\n",
        "    \"https://www.youtube.com/watch?v=2p4V1ZCQneo&pp=0gcJCfsJAYcqIYzv\",\n",
        "    \"https://www.youtube.com/watch?v=oNX7mhHewG0\",\n",
        "    \"https://www.youtube.com/watch?v=_QXWa7QaEgk\",\n",
        "    \"https://www.youtube.com/watch?v=xHdQYKq1LMs\",\n",
        "    \"https://www.youtube.com/watch?v=K2OsUADOtLc\",\n",
        "    \"https://www.youtube.com/watch?v=G4e9vrU1lrc\",\n",
        "    \"https://www.youtube.com/watch?v=Zj0hcokB5Lg&pp=0gcJCfsJAYcqIYzv\",\n",
        "    \"https://www.youtube.com/watch?v=uxXRtgQfEFI\",\n",
        "    \"https://www.youtube.com/watch?v=M8c_NoP01_A\",\n",
        "    \"https://www.youtube.com/watch?v=kD28gc6oqV4\",\n",
        "    \"https://www.youtube.com/watch?v=0xUbMicNy-w\",\n",
        "    \"https://www.youtube.com/watch?v=if2P7EPOgsY\",\n",
        "    ]\n",
        "\n",
        "    # Path to your YouTube cookies file (optional, for restricted videos)\n",
        "    cookie_file_path = 'www.youtube.com_cookies.txt'\n",
        "    if not os.path.exists(cookie_file_path):\n",
        "        print(f\"‚ö†Ô∏è Cookie file not found at '{cookie_file_path}'. Restricted content may fail.\")\n",
        "        cookie_file_path = None\n",
        "\n",
        "    print(\"\\n--- Processing Video URLs ---\")\n",
        "\n",
        "    # New: Create a list to hold all individual video URLs\n",
        "    all_individual_urls = []\n",
        "\n",
        "    # New: yt-dlp options just for info extraction to find videos in playlists\n",
        "    # 'noplaylist': False (default) is needed to process playlists.\n",
        "    info_opts = {\n",
        "        'extract_flat': 'in_playlist', # Get entries without full info\n",
        "        'skip_download': True,\n",
        "        'quiet': True,\n",
        "        'no_warnings': True, # Suppress warnings (like SABR)\n",
        "    }\n",
        "    if cookie_file_path:\n",
        "        info_opts['cookiefile'] = cookie_file_path\n",
        "\n",
        "    print(\"Inspecting provided URLs for playlists...\")\n",
        "    with yt_dlp.YoutubeDL(info_opts) as ydl:\n",
        "        for url in video_urls:\n",
        "            print(f\"Inspecting: {url}\")\n",
        "            try:\n",
        "                # Extract info\n",
        "                info = ydl.extract_info(url, download=False)\n",
        "\n",
        "                # Check if it's a playlist\n",
        "                if info.get('_type') == 'playlist':\n",
        "                    print(f\"  -> üîó Found playlist: {info.get('title', 'Unknown Playlist')}\")\n",
        "                    # Extract all video URLs from the playlist entries\n",
        "                    playlist_video_urls = [entry.get('url') for entry in info.get('entries', []) if entry and entry.get('url')]\n",
        "                    all_individual_urls.extend(playlist_video_urls)\n",
        "                    print(f\"  -> Added {len(playlist_video_urls)} videos from playlist.\")\n",
        "                else:\n",
        "                    # It's a single video, add its original URL\n",
        "                    print(\"  -> Single video found.\")\n",
        "                    all_individual_urls.append(url)\n",
        "\n",
        "            except yt_dlp.utils.DownloadError as e:\n",
        "                print(f\"  -> ‚ùå Error inspecting URL {url}: {e}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  -> ‚ùå Unexpected error inspecting URL {url}: {e}\")\n",
        "\n",
        "    print(f\"\\n--- Total individual videos to process: {len(all_individual_urls)} ---\")\n",
        "\n",
        "    # Now, process each individual URL\n",
        "    for i, video_url in enumerate(all_individual_urls):\n",
        "        download_video_and_extract_audio(\n",
        "            video_url,\n",
        "            output_dir=VIDEOS_DIRECTORY,\n",
        "            audio_dir=AUDIO_DIRECTORY,\n",
        "            metadata_file=METADATA_FILE,\n",
        "            cookie_file=cookie_file_path\n",
        "        )\n",
        "\n",
        "        # Add a sleep timer after each download, except for the last one\n",
        "        if i < len(all_individual_urls) - 1:\n",
        "            print(f\"\\n--- üò¥ Sleeping for 1 seconds before next download ({i+2}/{len(all_individual_urls)}) ---\")\n",
        "            time.sleep(1)\n",
        "\n",
        "    print(\"\\nURL processing batch completed.\")\n",
        "\n",
        "    # Run verification for any videos that might have failed audio extraction\n",
        "    verify_and_process_existing_videos(VIDEOS_DIRECTORY, AUDIO_DIRECTORY)\n",
        "\n",
        "    print(\"\\n--- All processing finished ---\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove Videos and Audios more than a specific threshold like 20 minutes (1200 seconds)"
      ],
      "metadata": {
        "id": "AVB4dIMe5N4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import sys\n",
        "\n",
        "# --- Configuration ---\n",
        "# Define the maximum allowed duration in seconds (e.g., 20 minutes = 20 * 60 = 1200)\n",
        "MAX_DURATION_SECONDS = 1500\n",
        "\n",
        "# Define paths (must match your main script)\n",
        "VIDEOS_DIRECTORY = './videos'\n",
        "AUDIO_DIRECTORY = './audio'\n",
        "METADATA_FILE = os.path.join(VIDEOS_DIRECTORY, 'video_metadata.csv')\n",
        "# --- End Configuration ---\n",
        "\n",
        "\n",
        "def cleanup_long_videos(metadata_path, videos_dir, audio_dir, max_seconds):\n",
        "    \"\"\"\n",
        "    Scans a metadata CSV and removes video/audio files that exceed a\n",
        "    duration threshold. Updates the metadata file.\n",
        "    \"\"\"\n",
        "    print(\"--- Video Cleanup Utility ---\")\n",
        "\n",
        "    if not os.path.exists(metadata_path):\n",
        "        print(f\"‚ùå Error: Metadata file not found at '{metadata_path}'. Cannot proceed.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(metadata_path)\n",
        "    except pd.errors.EmptyDataError:\n",
        "        print(\"‚úÖ Metadata file is empty. Nothing to clean up.\")\n",
        "        return\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error reading metadata file: {e}\")\n",
        "        return\n",
        "\n",
        "    # Ensure 'duration_seconds' column exists\n",
        "    if 'duration_seconds' not in df.columns:\n",
        "        print(\"‚ùå Error: 'duration_seconds' column not found in metadata.\")\n",
        "        return\n",
        "\n",
        "    # Convert duration to numeric, handling errors (like 'N/A' or 'FAILED')\n",
        "    # 'coerce' will turn non-numeric values into 'NaT' (Not a Time) / 'NaN' (Not a Number)\n",
        "    df['duration_numeric'] = pd.to_numeric(df['duration_seconds'], errors='coerce')\n",
        "\n",
        "    # Find videos to keep vs. videos to remove\n",
        "    # Keep videos that are within the threshold (or have unknown duration)\n",
        "    # We use .fillna(0) so that 'NaN' values (unknown duration) are kept\n",
        "    to_keep_mask = df['duration_numeric'].fillna(0) <= max_seconds\n",
        "\n",
        "    df_to_keep = df[to_keep_mask]\n",
        "    df_to_remove = df[~to_keep_mask]\n",
        "\n",
        "    if df_to_remove.empty:\n",
        "        print(f\"‚úÖ No videos found exceeding the {max_seconds}s threshold.\")\n",
        "        # Clean up the temporary column just in case\n",
        "        if 'duration_numeric' in df.columns:\n",
        "             df = df.drop(columns=['duration_numeric'])\n",
        "             df.to_csv(metadata_path, index=False)\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(df_to_remove)} video(s) to remove (duration > {max_seconds}s):\")\n",
        "\n",
        "    # New: List videos before asking for confirmation\n",
        "    print(\"\\n--- Videos to be removed ---\")\n",
        "    for index, row in df_to_remove.iterrows():\n",
        "        title = row.get('title', f\"URL: {row.get('url', 'N/A')}\")\n",
        "        duration = row.get('duration_seconds', 'N/A')\n",
        "        print(f\"  - {title} (Duration: {duration}s)\")\n",
        "    print(\"------------------------------\")\n",
        "\n",
        "    # New: Ask for confirmation here, inside the function\n",
        "    confirm = input(\"\\nAre you sure you want to proceed with deleting these files and entries? (yes/no): \")\n",
        "    if confirm.lower() != 'yes':\n",
        "        print(\"Operation cancelled by user.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nProceeding with deletion...\")\n",
        "    removed_count = 0\n",
        "    for index, row in df_to_remove.iterrows():\n",
        "        video_name = row.get('filename')\n",
        "        audio_name = row.get('audio_filename')\n",
        "        title = row.get('title', f\"URL: {row.get('url', 'N/A')}\")\n",
        "\n",
        "        print(f\"\\nProcessing '{title}' (Duration: {row.get('duration_seconds')}s)\")\n",
        "\n",
        "        # 1. Remove Video File\n",
        "        if pd.notna(video_name) and video_name not in [\"FAILED\", \"SKIPPED_DURATION\"]:\n",
        "            video_path = os.path.join(videos_dir, video_name)\n",
        "            if os.path.exists(video_path):\n",
        "                try:\n",
        "                    os.remove(video_path)\n",
        "                    print(f\"  üóëÔ∏è Removed video: {video_path}\")\n",
        "                    removed_count += 1\n",
        "                except Exception as e:\n",
        "                    print(f\"  ‚ùå Error removing video {video_path}: {e}\")\n",
        "            else:\n",
        "                print(f\"  ü§∑ Video file not found: {video_path}\")\n",
        "        else:\n",
        "            print(f\"  ‚ÑπÔ∏è No valid video filename listed.\")\n",
        "\n",
        "        # 2. Remove Audio File\n",
        "        if pd.notna(audio_name) and audio_name not in [\"FAILED\", \"SKIPPED_DURATION\"]:\n",
        "            audio_path = os.path.join(audio_dir, audio_name)\n",
        "            if os.path.exists(audio_path):\n",
        "                try:\n",
        "                    os.remove(audio_path)\n",
        "                    print(f\"  üóëÔ∏è Removed audio: {audio_path}\")\n",
        "                    removed_count += 1\n",
        "                except Exception as e:\n",
        "                    print(f\"  ‚ùå Error removing audio {audio_path}: {e}\")\n",
        "            else:\n",
        "                print(f\"  ü§∑ Audio file not found: {audio_path}\")\n",
        "        else:\n",
        "            print(f\"  ‚ÑπÔ∏è No valid audio filename listed.\")\n",
        "\n",
        "    # 3. Update the metadata CSV file\n",
        "    try:\n",
        "        # Drop the temporary column before saving\n",
        "        df_to_keep = df_to_keep.drop(columns=['duration_numeric'])\n",
        "        df_to_keep.to_csv(metadata_path, index=False)\n",
        "        print(f\"\\n‚úÖ Successfully updated metadata file: {metadata_path}\")\n",
        "        print(f\"Removed {len(df_to_remove)} entries from CSV.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå CRITICAL: Error writing updated metadata file: {e}\")\n",
        "        print(\"   Your files may be deleted, but the CSV was not updated.\")\n",
        "\n",
        "    print(f\"\\n--- Cleanup Summary ---\")\n",
        "    print(f\"Removed {len(df_to_remove)} videos from metadata.\")\n",
        "    print(f\"Deleted {removed_count} associated files.\")\n",
        "    print(\"--- Cleanup Finished ---\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Updated main block\n",
        "    print(f\"--- Video Cleanup Utility ---\")\n",
        "    print(f\"This script will find files over {MAX_DURATION_SECONDS} seconds.\")\n",
        "    print(f\"It will read from: {METADATA_FILE}\")\n",
        "    print(f\"It will look for files in: {VIDEOS_DIRECTORY} and {AUDIO_DIRECTORY}\")\n",
        "    print(\"You will be asked for confirmation before any files are deleted.\")\n",
        "\n",
        "    # Check if a command-line argument is provided to auto-confirm\n",
        "    if len(sys.argv) > 1 and sys.argv[1].lower() == '--yes':\n",
        "        print(\"\\n'--yes' flag detected, auto-confirming...\")\n",
        "        # This part is for automation, but the main logic is now inside the function\n",
        "        # We'll just call the function, but the function itself will now ask.\n",
        "        # Let's adjust the logic. The user *probably* wants --yes to bypass the *new* check.\n",
        "\n",
        "        # Let's re-think the main block logic to better support --yes\n",
        "\n",
        "        # We need to pass the confirmation status *into* the function.\n",
        "        # I will refactor.\n",
        "        pass # Will rewrite the main block and function slightly.\n",
        "\n",
        "\n",
        "# --- Let's refactor the code to handle the confirmation logic better ---\n",
        "\n",
        "def find_videos_to_remove(metadata_path, max_seconds):\n",
        "    \"\"\"Finds videos over the duration without deleting.\"\"\"\n",
        "    if not os.path.exists(metadata_path):\n",
        "        print(f\"‚ùå Error: Metadata file not found at '{metadata_path}'. Cannot proceed.\")\n",
        "        return None, None\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(metadata_path)\n",
        "    except pd.errors.EmptyDataError:\n",
        "        print(\"‚úÖ Metadata file is empty. Nothing to clean up.\")\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error reading metadata file: {e}\")\n",
        "        return None, None\n",
        "\n",
        "    if 'duration_seconds' not in df.columns:\n",
        "        print(\"‚ùå Error: 'duration_seconds' column not found in metadata.\")\n",
        "        return None, None\n",
        "\n",
        "    df['duration_numeric'] = pd.to_numeric(df['duration_seconds'], errors='coerce')\n",
        "    to_keep_mask = df['duration_numeric'].fillna(0) <= max_seconds\n",
        "\n",
        "    df_to_keep = df[to_keep_mask]\n",
        "    df_to_remove = df[~to_keep_mask]\n",
        "\n",
        "    return df_to_keep, df_to_remove\n",
        "\n",
        "\n",
        "def delete_videos(df_to_remove, df_to_keep, metadata_path, videos_dir, audio_dir):\n",
        "    \"\"\"Performs the actual deletion of files and updates the CSV.\"\"\"\n",
        "\n",
        "    print(\"\\nProceeding with deletion...\")\n",
        "    removed_count = 0\n",
        "\n",
        "    for index, row in df_to_remove.iterrows():\n",
        "        video_name = row.get('filename')\n",
        "        audio_name = row.get('audio_filename')\n",
        "        title = row.get('title', f\"URL: {row.get('url', 'N/A')}\")\n",
        "\n",
        "        print(f\"\\nProcessing '{title}' (Duration: {row.get('duration_seconds')}s)\")\n",
        "\n",
        "        # 1. Remove Video File\n",
        "        if pd.notna(video_name) and video_name not in [\"FAILED\", \"SKIPPED_DURATION\"]:\n",
        "            video_path = os.path.join(videos_dir, video_name)\n",
        "            if os.path.exists(video_path):\n",
        "                try:\n",
        "                    os.remove(video_path)\n",
        "                    print(f\"  üóëÔ∏è Removed video: {video_path}\")\n",
        "                    removed_count += 1\n",
        "                except Exception as e:\n",
        "                    print(f\"  ‚ùå Error removing video {video_path}: {e}\")\n",
        "            else:\n",
        "                print(f\"  ü§∑ Video file not found: {video_path}\")\n",
        "        else:\n",
        "            print(f\"  ‚ÑπÔ∏è No valid video filename listed.\")\n",
        "\n",
        "        # 2. Remove Audio File\n",
        "        if pd.notna(audio_name) and audio_name not in [\"FAILED\", \"SKIPPED_DURATION\"]:\n",
        "            audio_path = os.path.join(audio_dir, audio_name)\n",
        "            if os.path.exists(audio_path):\n",
        "                try:\n",
        "                    os.remove(audio_path)\n",
        "                    print(f\"  üóëÔ∏è Removed audio: {audio_path}\")\n",
        "                    removed_count += 1\n",
        "                except Exception as e:\n",
        "                    print(f\"  ‚ùå Error removing audio {audio_path}: {e}\")\n",
        "            else:\n",
        "                print(f\"  ü§∑ Audio file not found: {audio_path}\")\n",
        "        else:\n",
        "            print(f\"  ‚ÑπÔ∏è No valid audio filename listed.\")\n",
        "\n",
        "    # 3. Update the metadata CSV file\n",
        "    try:\n",
        "        # Drop the temporary column before saving\n",
        "        if 'duration_numeric' in df_to_keep.columns:\n",
        "            df_to_keep = df_to_keep.drop(columns=['duration_numeric'])\n",
        "\n",
        "        df_to_keep.to_csv(metadata_path, index=False)\n",
        "        print(f\"\\n‚úÖ Successfully updated metadata file: {metadata_path}\")\n",
        "        print(f\"Removed {len(df_to_remove)} entries from CSV.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå CRITICAL: Error writing updated metadata file: {e}\")\n",
        "        print(\"   Your files may be deleted, but the CSV was not updated.\")\n",
        "\n",
        "    print(f\"\\n--- Cleanup Summary ---\")\n",
        "    print(f\"Removed {len(df_to_remove)} videos from metadata.\")\n",
        "    print(f\"Deleted {removed_count} associated files.\")\n",
        "    print(\"--- Cleanup Finished ---\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print(f\"--- Video Cleanup Utility ---\")\n",
        "    print(f\"This script will find files over {MAX_DURATION_SECONDS} seconds.\")\n",
        "    print(f\"It will read from: {METADATA_FILE}\")\n",
        "    print(f\"It will look for files in: {VIDEOS_DIRECTORY} and {AUDIO_DIRECTORY}\")\n",
        "\n",
        "    # 1. Find videos\n",
        "    df_to_keep, df_to_remove = find_videos_to_remove(METADATA_FILE, MAX_DURATION_SECONDS)\n",
        "\n",
        "    # 2. Check results\n",
        "    if df_to_remove is None or df_to_remove.empty:\n",
        "        if df_to_remove is not None: # This means it was empty, not an error\n",
        "             print(f\"‚úÖ No videos found exceeding the {MAX_DURATION_SECONDS}s threshold.\")\n",
        "             # We might need to save the DF to remove the temp column\n",
        "             if df_to_keep is not None and 'duration_numeric' in df_to_keep.columns:\n",
        "                 df_to_keep = df_to_keep.drop(columns=['duration_numeric'])\n",
        "                 df_to_keep.to_csv(METADATA_FILE, index=False)\n",
        "        sys.exit() # Exit script\n",
        "\n",
        "    # 3. List videos\n",
        "    print(f\"\\nFound {len(df_to_remove)} video(s) to remove (duration > {MAX_DURATION_SECONDS}s):\")\n",
        "    print(\"------------------------------\")\n",
        "    for index, row in df_to_remove.iterrows():\n",
        "        title = row.get('title', f\"URL: {row.get('url', 'N/A')}\")\n",
        "        duration = row.get('duration_seconds', 'N/A')\n",
        "        print(f\"  - {title} (Duration: {duration}s)\")\n",
        "    print(\"------------------------------\")\n",
        "\n",
        "    # 4. Check for auto-confirmation or ask user\n",
        "    auto_confirm = len(sys.argv) > 1 and sys.argv[1].lower() == '--yes'\n",
        "\n",
        "    if auto_confirm:\n",
        "        print(\"\\n'--yes' flag detected, proceeding with deletion...\")\n",
        "        proceed = True\n",
        "    else:\n",
        "        confirm = input(\"\\nAre you sure you want to proceed with deleting these files and entries? (yes/no): \")\n",
        "        proceed = confirm.lower() == 'yes'\n",
        "\n",
        "    # 5. Execute deletion if confirmed\n",
        "    if proceed:\n",
        "        delete_videos(df_to_remove, df_to_keep, METADATA_FILE, VIDEOS_DIRECTORY, AUDIO_DIRECTORY)\n",
        "    else:\n",
        "        print(\"Operation cancelled by user.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNMCmgSA5NsJ",
        "outputId": "d9d217e9-1e52-4c54-dec3-60b81928b132"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Video Cleanup Utility ---\n",
            "This script will find files over 1500 seconds.\n",
            "It will read from: ./videos/video_metadata.csv\n",
            "It will look for files in: ./videos and ./audio\n",
            "You will be asked for confirmation before any files are deleted.\n",
            "--- Video Cleanup Utility ---\n",
            "This script will find files over 1500 seconds.\n",
            "It will read from: ./videos/video_metadata.csv\n",
            "It will look for files in: ./videos and ./audio\n",
            "\n",
            "Found 1 video(s) to remove (duration > 1500s):\n",
            "------------------------------\n",
            "  - Understanding cataract and lens surgery.  How we explain it.  Shannon Wong, MD (Duration: 1643s)\n",
            "------------------------------\n",
            "\n",
            "Are you sure you want to proceed with deleting these files and entries? (yes/no): no\n",
            "Operation cancelled by user.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"CUDA memory cleared.\")\n",
        "else:\n",
        "    print(\"CUDA not available.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsjGuu_d1byJ",
        "outputId": "745a9e0b-42ee-4b35-b40f-033982f8ed5b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA memory cleared.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/openai/whisper.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-h7V_Cb2Jom",
        "outputId": "d1caa6e5-038c-41fa-ea5f-536b9cd0d69b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install git+https://github.com/openai/whisper.git\n",
        "\n",
        "import os\n",
        "import whisper\n",
        "import torch\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "def transcribe_audio_files(input_dir: str = './audio', output_dir: str = './transcripts'):\n",
        "    \"\"\"\n",
        "    Transcribes all .wav files in the input directory using Whisper's large-v3 model,\n",
        "    capturing sentence-level timestamps, and saves the output as .json files.\n",
        "\n",
        "      Args:\n",
        "        input_dir: The directory containing the .wav files (16kHz mono).\n",
        "        output_dir: The directory where the transcription .json files will be saved.\n",
        "    \"\"\"\n",
        "    print(\"--- Starting Audio Transcription Process ---\")\n",
        "\n",
        "    # 1. Setup directories and check for GPU\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f\"Using device: {device.upper()}\")\n",
        "    if device == 'cpu':\n",
        "        print(\"‚ö†Ô∏è WARNING: No GPU found. Transcription will be very slow.\")\n",
        "\n",
        "    # 2. Load the pre-trained Whisper model\n",
        "    print(\"Loading Whisper model (large)...\")\n",
        "    try:\n",
        "        model = whisper.load_model(\"large\", device=device)\n",
        "        print(\"‚úÖ Model loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading Whisper model: {e}\")\n",
        "        print(\"Please check your internet connection and if 'openai-whisper' is installed correctly.\")\n",
        "        return\n",
        "\n",
        "    # 3. Identify audio files to process\n",
        "    audio_files = {os.path.splitext(f)[0] for f in os.listdir(input_dir) if f.endswith('.wav')}\n",
        "    transcribed_files = {os.path.splitext(f)[0] for f in os.listdir(output_dir) if f.endswith('.json')}\n",
        "    files_to_process = sorted([f + '.wav' for f in (audio_files - transcribed_files)])\n",
        "\n",
        "    if not files_to_process:\n",
        "        print(\"‚úÖ All audio files have already been transcribed.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(files_to_process)} audio file(s) to transcribe.\")\n",
        "\n",
        "    # 4. Process each audio file\n",
        "    for filename in tqdm(files_to_process, desc=\"Transcribing Audio\"):\n",
        "        input_path = os.path.join(input_dir, filename)\n",
        "        output_filename = f\"{os.path.splitext(filename)[0]}.json\"\n",
        "        output_path = os.path.join(output_dir, output_filename)\n",
        "\n",
        "        try:\n",
        "            # Perform transcription (sentence/segment-level timestamps by default)\n",
        "            result = model.transcribe(input_path, fp16=torch.cuda.is_available())\n",
        "\n",
        "            # Extract sentence-level segments and round timestamps to one decimal place\n",
        "            segments = [\n",
        "                {\n",
        "                    \"start\": round(seg[\"start\"], 1),\n",
        "                    \"end\": round(seg[\"end\"], 1),\n",
        "                    \"text\": seg[\"text\"].strip()\n",
        "                }\n",
        "                for seg in result[\"segments\"]\n",
        "            ]\n",
        "\n",
        "            # Save just the clean sentence-level transcript with timestamps\n",
        "            with open(output_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(segments, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "        except Exception as e:\n",
        "            tqdm.write(f\"‚ùå Error transcribing {filename}: {e}\")\n",
        "\n",
        "    print(\"\\n--- Audio Transcription process completed. ---\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Make sure ffmpeg is installed on your system and accessible in your PATH.\n",
        "    # On Debian /Ubuntu: sudo apt update && sudo apt install ffmpeg\n",
        "    # On macOS (using Homebrew): brew install ffmpeg\n",
        "    # On Windows (using Chocolatey): choco install ffmpeg\n",
        "    transcribe_audio_files()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcLT3SUT2OOA",
        "outputId": "e24edc14-213b-4c1c-91e8-5334fba9d349"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Audio Transcription Process ---\n",
            "Using device: CUDA\n",
            "Loading Whisper model (large)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.88G/2.88G [00:39<00:00, 78.1MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model loaded successfully.\n",
            "Found 65 audio file(s) to transcribe.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Transcribing Audio:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 46/65 [1:39:09<50:51, 160.60s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lTnnINX649fh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}