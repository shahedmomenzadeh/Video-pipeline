{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9Iy2psGagD6"
      },
      "source": [
        "# Pre training video dataset creating from YouTube videos\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxRJdEmaafkh",
        "outputId": "b8ae07ed-e99d-42f7-c7b0-60e55a844411"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/pre_training_phase_data\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/pre_training_phase_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffU-Juy2qcp-",
        "outputId": "7aa212d8-8024-43f9-be7c-98f232e6ef40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "audio  transcripts  videos  www.youtube.com_cookies.txt\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLibrFKFbx4N",
        "outputId": "848fcdb7-8753-43c2-9cc2-2aa5b367cf93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Videos 250\n",
            "Number of Audios 246\n",
            "Number of transcripts 181\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "videos_list = os.listdir(\"./videos\")\n",
        "print(f\"Number of Videos {len(videos_list)}\")\n",
        "print(f\"Number of Audios {len(os.listdir('./audio'))}\")\n",
        "print(f\"Number of transcripts {len(os.listdir('./transcripts'))}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3awSaZHXZmiN"
      },
      "source": [
        "# Download YouTube Videos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ii0I2iJdZ2Of",
        "outputId": "f13d56f9-e408-4be3-b183-2175b7e65b38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/176.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m176.0/176.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/3.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m140.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q yt-dlp pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiKw1th1HfUY"
      },
      "outputs": [],
      "source": [
        "import yt_dlp\n",
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import shutil\n",
        "import subprocess\n",
        "import time  # Import the time module\n",
        "\n",
        "\n",
        "def is_ffmpeg_installed():\n",
        "    \"\"\"Check if FFmpeg is installed and available in the system's PATH.\"\"\"\n",
        "    return shutil.which(\"ffmpeg\") is not None\n",
        "\n",
        "\n",
        "def extract_audio_ffmpeg(video_filepath: str, audio_dir: str) -> str | None:\n",
        "    \"\"\"\n",
        "    Extracts audio from a video file using FFmpeg, converting it to 16kHz mono WAV.\n",
        "\n",
        "    Args:\n",
        "        video_filepath: The full path to the input video file.\n",
        "        audio_dir: The directory where the extracted audio will be saved.\n",
        "\n",
        "    Returns:\n",
        "        The filename of the extracted audio file if successful, otherwise None.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(video_filepath):\n",
        "        print(f\"‚ùå Error: Video file not found at {video_filepath}\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        video_basename = os.path.basename(video_filepath)\n",
        "        video_name_no_ext = os.path.splitext(video_basename)[0]\n",
        "        audio_filename = f\"{video_name_no_ext}.wav\"\n",
        "        output_audio_path = os.path.join(audio_dir, audio_filename)\n",
        "\n",
        "        print(f\"üéµ Extracting audio from '{video_basename}'...\")\n",
        "\n",
        "        # Command to extract audio, convert to PCM 16-bit little-endian,\n",
        "        # set sample rate to 16kHz, mono channel, and overwrite output\n",
        "        command = [\n",
        "            'ffmpeg', '-i', video_filepath, '-vn', '-acodec', 'pcm_s16le',\n",
        "            '-ar', '16000', '-ac', '1', '-y', output_audio_path\n",
        "        ]\n",
        "\n",
        "        # Run ffmpeg, suppressing stdout and stderr to keep the log clean\n",
        "        subprocess.run(command, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "        print(f\"‚úÖ Audio extracted: {output_audio_path}\")\n",
        "        return audio_filename\n",
        "\n",
        "    except subprocess.CalledProcessError:\n",
        "        print(f\"‚ùå FFmpeg error during audio extraction for {video_filepath}.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Unexpected error during audio extraction: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def download_video_and_extract_audio(video_url: str,\n",
        "                                     output_dir: str = './videos',\n",
        "                                     audio_dir: str = './audio',\n",
        "                                     metadata_file: str = './videos/video_metadata.csv',\n",
        "                                     cookie_file: str | None = None):\n",
        "    \"\"\"\n",
        "    Downloads a YouTube video, extracts its audio, logs metadata, and skips processed videos.\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    os.makedirs(audio_dir, exist_ok=True)\n",
        "\n",
        "    # Define the metadata columns\n",
        "    metadata_columns = [\n",
        "        'title', 'channel_name', 'url', 'filename',\n",
        "        'download_date', 'duration_seconds', 'resolution', 'audio_filename'\n",
        "    ]\n",
        "\n",
        "    # Load or initialize metadata DataFrame\n",
        "    if os.path.exists(metadata_file):\n",
        "        try:\n",
        "            metadata_df = pd.read_csv(metadata_file)\n",
        "            # Ensure all required columns exist (for backward compatibility)\n",
        "            for col in metadata_columns:\n",
        "                if col not in metadata_df.columns:\n",
        "                    metadata_df[col] = None\n",
        "            # Reorder columns for consistency\n",
        "            metadata_df = metadata_df[metadata_columns]\n",
        "        except pd.errors.EmptyDataError:\n",
        "            metadata_df = pd.DataFrame(columns=metadata_columns)\n",
        "    else:\n",
        "        metadata_df = pd.DataFrame(columns=metadata_columns)\n",
        "\n",
        "    # Skip if video URL already processed\n",
        "    if video_url in metadata_df['url'].values:\n",
        "        print(f\"‚è© Video already in metadata (skipped): {video_url}\")\n",
        "        return\n",
        "\n",
        "    # yt-dlp options\n",
        "    ydl_opts = {\n",
        "        # Get 480p video + best audio, merge into mp4\n",
        "        'format': 'bestvideo[height=480][ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best',\n",
        "        'outtmpl': os.path.join(output_dir, '%(title)s.%(ext)s'),\n",
        "        'noplaylist': True,\n",
        "        'merge_output_format': 'mp4',\n",
        "        'postprocessors': [{'key': 'FFmpegMetadata', 'add_chapters': False}],\n",
        "        'retries': 5,\n",
        "        'fragment_retries': 5,\n",
        "        'no_warnings': True, # Suppress warnings (like SABR)\n",
        "    }\n",
        "\n",
        "    if cookie_file:\n",
        "        ydl_opts['cookiefile'] = cookie_file\n",
        "\n",
        "    try:\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            # Extract info first without downloading\n",
        "            info = ydl.extract_info(video_url, download=False)\n",
        "            video_title = str(info.get('title', 'Unknown Title'))\n",
        "            channel_name = info.get('uploader', 'Unknown Channel')\n",
        "            duration = info.get('duration')\n",
        "            width, height = info.get('width'), info.get('height')\n",
        "            resolution = f\"{width}x{height}\" if width and height else \"N/A\"\n",
        "\n",
        "            # Get the expected downloaded video path\n",
        "            expected_video_path = ydl.prepare_filename(info)\n",
        "            video_name_no_ext = os.path.splitext(os.path.basename(expected_video_path))[0]\n",
        "            expected_audio_filename = f\"{video_name_no_ext}.wav\"\n",
        "            expected_audio_path = os.path.join(audio_dir, expected_audio_filename)\n",
        "\n",
        "            # Skip if audio file already exists\n",
        "            if os.path.exists(expected_audio_path):\n",
        "                print(f\"‚è© Audio already exists, assuming processed: {expected_audio_filename}\")\n",
        "                # Log metadata if it was missing (e.g., script interrupted)\n",
        "                if video_url not in metadata_df['url'].values:\n",
        "                    new_entry = pd.DataFrame([{\n",
        "                        'title': video_title,\n",
        "                        'channel_name': channel_name,\n",
        "                        'url': video_url,\n",
        "                        'filename': os.path.basename(expected_video_path),\n",
        "                        'download_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                        'duration_seconds': duration,\n",
        "                        'resolution': resolution,\n",
        "                        'audio_filename': expected_audio_filename\n",
        "                    }])\n",
        "                    metadata_df = pd.concat([metadata_df, new_entry], ignore_index=True)\n",
        "                    metadata_df.to_csv(metadata_file, index=False)\n",
        "                return\n",
        "\n",
        "            print(f\"‚¨áÔ∏è Downloading: '{video_title}' from channel: {channel_name}\")\n",
        "            ydl.download([video_url])\n",
        "\n",
        "            # Verify download and extract audio\n",
        "            if os.path.exists(expected_video_path):\n",
        "                print(f\"‚úÖ Download complete: {os.path.basename(expected_video_path)}\")\n",
        "                audio_filename = extract_audio_ffmpeg(expected_video_path, audio_dir)\n",
        "\n",
        "                # Log new entry to metadata\n",
        "                new_entry = pd.DataFrame([{\n",
        "                    'title': video_title,\n",
        "                    'channel_name': channel_name,\n",
        "                    'url': video_url,\n",
        "                    'filename': os.path.basename(expected_video_path),\n",
        "                    'download_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                    'duration_seconds': duration,\n",
        "                    'resolution': resolution,\n",
        "                    'audio_filename': audio_filename if audio_filename else \"N/A\"\n",
        "                }])\n",
        "                metadata_df = pd.concat([metadata_df, new_entry], ignore_index=True)\n",
        "                metadata_df.to_csv(metadata_file, index=False)\n",
        "            else:\n",
        "                print(f\"‚ùå Download reported success but file not found at '{expected_video_path}'\")\n",
        "\n",
        "    except yt_dlp.utils.DownloadError as e:\n",
        "        print(f\"‚ùå yt-dlp Download Error for {video_url}: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Unexpected error for {video_url}: {e}\")\n",
        "\n",
        "\n",
        "def verify_and_process_existing_videos(videos_dir: str, audio_dir: str):\n",
        "    \"\"\"\n",
        "    Scans the videos directory and extracts audio for any video missing its corresponding .wav file.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Verifying Existing Videos ---\")\n",
        "    if not os.path.isdir(videos_dir):\n",
        "        print(f\"‚ùå Verification skipped: '{videos_dir}' not found\")\n",
        "        return\n",
        "\n",
        "    # Get a set of audio filenames (without extension)\n",
        "    existing_audio_names = {os.path.splitext(f)[0] for f in os.listdir(audio_dir) if f.endswith('.wav')}\n",
        "    video_files = [f for f in os.listdir(videos_dir) if f.endswith(('.mp4', '.mkv', '.webm', '.mov'))]\n",
        "\n",
        "    # Find videos where the filename (without extension) is not in the audio set\n",
        "    missing_audio_videos = [\n",
        "        os.path.join(videos_dir, vf)\n",
        "        for vf in video_files\n",
        "        if os.path.splitext(vf)[0] not in existing_audio_names\n",
        "    ]\n",
        "\n",
        "    if not missing_audio_videos:\n",
        "        print(\"‚úÖ All videos have corresponding audio files.\")\n",
        "        return\n",
        "\n",
        "    print(f\"‚ö†Ô∏è Found {len(missing_audio_videos)} video(s) missing audio:\")\n",
        "    for v in missing_audio_videos:\n",
        "        print(f\"  - {os.path.basename(v)}\")\n",
        "\n",
        "    success, fail = 0, 0\n",
        "    for vpath in missing_audio_videos:\n",
        "        if extract_audio_ffmpeg(vpath, audio_dir):\n",
        "            success += 1\n",
        "        else:\n",
        "            fail += 1\n",
        "\n",
        "    print(\"\\n--- Verification Summary ---\")\n",
        "    print(f\"‚úÖ Extracted: {success}\")\n",
        "    print(f\"‚ùå Failed: {fail}\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Check for FFmpeg installation\n",
        "    if not is_ffmpeg_installed():\n",
        "        print(\"=\" * 60)\n",
        "        print(\"‚ö†Ô∏è FFmpeg is not installed or not in your system PATH.\")\n",
        "        print(\"   This script requires FFmpeg to extract audio.\")\n",
        "        print(\"   Download from: https://ffmpeg.org/download.html\")\n",
        "        print(\"=\" * 60)\n",
        "    else:\n",
        "        print(\"‚úÖ FFmpeg found.\")\n",
        "\n",
        "    # Define directories and files\n",
        "    VIDEOS_DIRECTORY = './videos'\n",
        "    AUDIO_DIRECTORY = './audio'\n",
        "    METADATA_FILE = os.path.join(VIDEOS_DIRECTORY, 'video_metadata.csv')\n",
        "\n",
        "    # List of videos to download\n",
        "    video_urls = [\n",
        "    # \"https://www.youtube.com/watch?v=OZ5SmpNFlU8&pp=ygUZY2F0YXJhY3Qgc3VyZ2VyeSBuYXJyYXRlZA%3D%3D\",\n",
        "    # \"https://youtube.com/playlist?list=PLjgfi4kp5BU7Lxk_O9GLCFzjXITHVonC0&si=ITWCQq8LzXfFRRKv\",\n",
        "    # \"https://youtube.com/playlist?list=PL88gdqtVPZep0oIpxOP1AStJPK0-Q2hEj&si=-NwuoIQ1VDi5M-Al\",\n",
        "    # \"https://youtube.com/playlist?list=PL88gdqtVPZeq0B1DAszU4uUmXbsksZsUf&si=vuURrNS4U10eE3hX\",\n",
        "    # \"https://youtube.com/playlist?list=PL88gdqtVPZepBwG8e9A4HMb-cy0DJmwvp&si=B6XLDmBGXNpYZ7GX\",\n",
        "    # \"https://www.youtube.com/playlist?list=PL88gdqtVPZerlilEVmKR-s3RTADHniLqu\",\n",
        "    # \"https://youtube.com/playlist?list=PLker0kXqgiOUV92h5wszstCLFQ-JFKATf&si=2OunF06tse7abgHt\",\n",
        "     \"https://youtube.com/playlist?list=PLker0kXqgiOVD5NNEES1UAYG7e-T6phV7&si=BQDJzsn_PHp_xFuK\",\n",
        "     \"https://youtu.be/-Q4uQ6rEExs?si=4mL-Ysen96rifmsb\",\n",
        "    \"https://youtu.be/LDhPCHvGxeA?si=xtkqrHo5gRzvUe4p\",\n",
        "    \"https://youtu.be/XjZ5r8GZq5Y?si=sr9ay6WbkXjx-RH2\",\n",
        "    \"https://youtu.be/pVTa7UhsyNc?si=2QFVOC-iNL_cSLRF\",\n",
        "    \"https://www.youtube.com/watch?v=PLSKmeAV43M\",\n",
        "    \"https://youtu.be/soJWJtHoplc?si=HROqo7yRTnzffEka\",\n",
        "    \"https://www.youtube.com/watch?v=PLSKmeAV43M\",\n",
        "    \"https://www.youtube.com/watch?v=QbeI72QmFAU&pp=ygUZY2F0YXJhY3Qgc3VyZ2VyeSBuYXJyYXRlZA%3D%3D\",\n",
        "    \"https://www.youtube.com/watch?v=6aIOKqBA-64\",\n",
        "    \"https://www.youtube.com/watch?v=yaAcqYn-Teo&pp=ygUZY2F0YXJhY3Qgc3VyZ2VyeSBuYXJyYXRlZA%3D%3D\",\n",
        "    \"https://www.youtube.com/watch?v=O60ZbtRcjik&pp=ygUZY2F0YXJhY3Qgc3VyZ2VyeSBuYXJyYXRlZA%3D%3D\",\n",
        "    \"https://www.youtube.com/watch?v=OZ5SmpNFlU8&pp=ygUZY2F0YXJhY3Qgc3VyZ2VyeSBuYXJyYXRlZA%3D%3D\",\n",
        "    \"https://www.youtube.com/watch?v=n_3cG9oeuNo&pp=ygUZY2F0YXJhY3Qgc3VyZ2VyeSBuYXJyYXRlZA%3D%3D\",\n",
        "    \"https://www.youtube.com/watch?v=-Q4uQ6rEExs&pp=ygUZY2F0YXJhY3Qgc3VyZ2VyeSBuYXJyYXRlZA%3D%3D\",\n",
        "    \"https://www.youtube.com/watch?v=moF1tUd9Flc&pp=ygUZY2F0YXJhY3Qgc3VyZ2VyeSBuYXJyYXRlZA%3D%3D\",\n",
        "    \"https://www.youtube.com/watch?v=aohAHNYpAOs&pp=ygUZY2F0YXJhY3Qgc3VyZ2VyeSBuYXJyYXRlZA%3D%3D\",\n",
        "    \"https://www.youtube.com/watch?v=SrzOrek2PVg\",\n",
        "    \"https://www.youtube.com/watch?v=wtw7m6C9HAw\",\n",
        "    \"https://www.youtube.com/watch?v=mfQXeuUcJdU&pp=0gcJCfsJAYcqIYzv\",\n",
        "    \"https://www.youtube.com/watch?v=s9CYeGi7ecs\",\n",
        "    \"https://www.youtube.com/watch?v=HPX8EBCVm_s\",\n",
        "    \"https://www.youtube.com/watch?v=vWCCDQgK06U&pp=0gcJCfsJAYcqIYzv\",\n",
        "    \"https://www.youtube.com/watch?v=2qEe2REdhWw\",\n",
        "    \"https://www.youtube.com/watch?v=3wwr5EzC0r4&pp=0gcJCfsJAYcqIYzv\",\n",
        "    \"https://www.youtube.com/watch?v=Q4ez4-t3WhE\",\n",
        "    \"https://www.youtube.com/watch?v=vWCCDQgK06U\",\n",
        "    \"https://www.youtube.com/watch?v=AV2ZRjYKpSA\",\n",
        "    \"https://www.youtube.com/watch?v=vfmohnIFMOQ\",\n",
        "    \"https://www.youtube.com/watch?v=2p4V1ZCQneo&pp=0gcJCfsJAYcqIYzv\",\n",
        "    \"https://www.youtube.com/watch?v=oNX7mhHewG0\",\n",
        "    \"https://www.youtube.com/watch?v=_QXWa7QaEgk\",\n",
        "    \"https://www.youtube.com/watch?v=xHdQYKq1LMs\",\n",
        "    \"https://www.youtube.com/watch?v=K2OsUADOtLc\",\n",
        "    \"https://www.youtube.com/watch?v=G4e9vrU1lrc\",\n",
        "    \"https://www.youtube.com/watch?v=Zj0hcokB5Lg&pp=0gcJCfsJAYcqIYzv\",\n",
        "    \"https://www.youtube.com/watch?v=uxXRtgQfEFI\",\n",
        "    \"https://www.youtube.com/watch?v=M8c_NoP01_A\",\n",
        "    \"https://www.youtube.com/watch?v=kD28gc6oqV4\",\n",
        "    \"https://www.youtube.com/watch?v=0xUbMicNy-w\",\n",
        "    \"https://www.youtube.com/watch?v=if2P7EPOgsY\",\n",
        "    ]\n",
        "\n",
        "    # Path to your YouTube cookies file (optional, for restricted videos)\n",
        "    cookie_file_path = 'www.youtube.com_cookies.txt'\n",
        "    if not os.path.exists(cookie_file_path):\n",
        "        print(f\"‚ö†Ô∏è Cookie file not found at '{cookie_file_path}'. Restricted content may fail.\")\n",
        "        cookie_file_path = None\n",
        "\n",
        "    print(\"\\n--- Processing Video URLs ---\")\n",
        "\n",
        "    # New: Create a list to hold all individual video URLs\n",
        "    all_individual_urls = []\n",
        "\n",
        "    # New: yt-dlp options just for info extraction to find videos in playlists\n",
        "    # 'noplaylist': False (default) is needed to process playlists.\n",
        "    info_opts = {\n",
        "        'extract_flat': 'in_playlist', # Get entries without full info\n",
        "        'skip_download': True,\n",
        "        'quiet': True,\n",
        "        'no_warnings': True, # Suppress warnings (like SABR)\n",
        "    }\n",
        "    if cookie_file_path:\n",
        "        info_opts['cookiefile'] = cookie_file_path\n",
        "\n",
        "    print(\"Inspecting provided URLs for playlists...\")\n",
        "    with yt_dlp.YoutubeDL(info_opts) as ydl:\n",
        "        for url in video_urls:\n",
        "            print(f\"Inspecting: {url}\")\n",
        "            try:\n",
        "                # Extract info\n",
        "                info = ydl.extract_info(url, download=False)\n",
        "\n",
        "                # Check if it's a playlist\n",
        "                if info.get('_type') == 'playlist':\n",
        "                    print(f\"  -> üîó Found playlist: {info.get('title', 'Unknown Playlist')}\")\n",
        "                    # Extract all video URLs from the playlist entries\n",
        "                    playlist_video_urls = [entry.get('url') for entry in info.get('entries', []) if entry and entry.get('url')]\n",
        "                    all_individual_urls.extend(playlist_video_urls)\n",
        "                    print(f\"  -> Added {len(playlist_video_urls)} videos from playlist.\")\n",
        "                else:\n",
        "                    # It's a single video, add its original URL\n",
        "                    print(\"  -> Single video found.\")\n",
        "                    all_individual_urls.append(url)\n",
        "\n",
        "            except yt_dlp.utils.DownloadError as e:\n",
        "                print(f\"  -> ‚ùå Error inspecting URL {url}: {e}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  -> ‚ùå Unexpected error inspecting URL {url}: {e}\")\n",
        "\n",
        "    print(f\"\\n--- Total individual videos to process: {len(all_individual_urls)} ---\")\n",
        "\n",
        "    # Now, process each individual URL\n",
        "    for i, video_url in enumerate(all_individual_urls):\n",
        "        download_video_and_extract_audio(\n",
        "            video_url,\n",
        "            output_dir=VIDEOS_DIRECTORY,\n",
        "            audio_dir=AUDIO_DIRECTORY,\n",
        "            metadata_file=METADATA_FILE,\n",
        "            cookie_file=cookie_file_path\n",
        "        )\n",
        "\n",
        "        # Add a sleep timer after each download, except for the last one\n",
        "        if i < len(all_individual_urls) - 1:\n",
        "            print(f\"\\n--- üò¥ Sleeping for 1 seconds before next download ({i+2}/{len(all_individual_urls)}) ---\")\n",
        "            time.sleep(1)\n",
        "\n",
        "    print(\"\\nURL processing batch completed.\")\n",
        "\n",
        "    # Run verification for any videos that might have failed audio extraction\n",
        "    verify_and_process_existing_videos(VIDEOS_DIRECTORY, AUDIO_DIRECTORY)\n",
        "\n",
        "    print(\"\\n--- All processing finished ---\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove Videos and Audios more than a specific threshold like 20 minutes (1200 seconds)"
      ],
      "metadata": {
        "id": "AVB4dIMe5N4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import sys\n",
        "\n",
        "# --- Configuration ---\n",
        "# Define the maximum allowed duration in seconds (e.g., 20 minutes = 20 * 60 = 1200)\n",
        "MAX_DURATION_SECONDS = 1500\n",
        "\n",
        "# Define paths (must match your main script)\n",
        "VIDEOS_DIRECTORY = './videos'\n",
        "AUDIO_DIRECTORY = './audio'\n",
        "METADATA_FILE = os.path.join(VIDEOS_DIRECTORY, 'video_metadata.csv')\n",
        "# --- End Configuration ---\n",
        "\n",
        "\n",
        "def cleanup_long_videos(metadata_path, videos_dir, audio_dir, max_seconds):\n",
        "    \"\"\"\n",
        "    Scans a metadata CSV and removes video/audio files that exceed a\n",
        "    duration threshold. Updates the metadata file.\n",
        "    \"\"\"\n",
        "    print(\"--- Video Cleanup Utility ---\")\n",
        "\n",
        "    if not os.path.exists(metadata_path):\n",
        "        print(f\"‚ùå Error: Metadata file not found at '{metadata_path}'. Cannot proceed.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(metadata_path)\n",
        "    except pd.errors.EmptyDataError:\n",
        "        print(\"‚úÖ Metadata file is empty. Nothing to clean up.\")\n",
        "        return\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error reading metadata file: {e}\")\n",
        "        return\n",
        "\n",
        "    # Ensure 'duration_seconds' column exists\n",
        "    if 'duration_seconds' not in df.columns:\n",
        "        print(\"‚ùå Error: 'duration_seconds' column not found in metadata.\")\n",
        "        return\n",
        "\n",
        "    # Convert duration to numeric, handling errors (like 'N/A' or 'FAILED')\n",
        "    # 'coerce' will turn non-numeric values into 'NaT' (Not a Time) / 'NaN' (Not a Number)\n",
        "    df['duration_numeric'] = pd.to_numeric(df['duration_seconds'], errors='coerce')\n",
        "\n",
        "    # Find videos to keep vs. videos to remove\n",
        "    # Keep videos that are within the threshold (or have unknown duration)\n",
        "    # We use .fillna(0) so that 'NaN' values (unknown duration) are kept\n",
        "    to_keep_mask = df['duration_numeric'].fillna(0) <= max_seconds\n",
        "\n",
        "    df_to_keep = df[to_keep_mask]\n",
        "    df_to_remove = df[~to_keep_mask]\n",
        "\n",
        "    if df_to_remove.empty:\n",
        "        print(f\"‚úÖ No videos found exceeding the {max_seconds}s threshold.\")\n",
        "        # Clean up the temporary column just in case\n",
        "        if 'duration_numeric' in df.columns:\n",
        "             df = df.drop(columns=['duration_numeric'])\n",
        "             df.to_csv(metadata_path, index=False)\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(df_to_remove)} video(s) to remove (duration > {max_seconds}s):\")\n",
        "\n",
        "    # New: List videos before asking for confirmation\n",
        "    print(\"\\n--- Videos to be removed ---\")\n",
        "    for index, row in df_to_remove.iterrows():\n",
        "        title = row.get('title', f\"URL: {row.get('url', 'N/A')}\")\n",
        "        duration = row.get('duration_seconds', 'N/A')\n",
        "        print(f\"  - {title} (Duration: {duration}s)\")\n",
        "    print(\"------------------------------\")\n",
        "\n",
        "    # New: Ask for confirmation here, inside the function\n",
        "    confirm = input(\"\\nAre you sure you want to proceed with deleting these files and entries? (yes/no): \")\n",
        "    if confirm.lower() != 'yes':\n",
        "        print(\"Operation cancelled by user.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nProceeding with deletion...\")\n",
        "    removed_count = 0\n",
        "    for index, row in df_to_remove.iterrows():\n",
        "        video_name = row.get('filename')\n",
        "        audio_name = row.get('audio_filename')\n",
        "        title = row.get('title', f\"URL: {row.get('url', 'N/A')}\")\n",
        "\n",
        "        print(f\"\\nProcessing '{title}' (Duration: {row.get('duration_seconds')}s)\")\n",
        "\n",
        "        # 1. Remove Video File\n",
        "        if pd.notna(video_name) and video_name not in [\"FAILED\", \"SKIPPED_DURATION\"]:\n",
        "            video_path = os.path.join(videos_dir, video_name)\n",
        "            if os.path.exists(video_path):\n",
        "                try:\n",
        "                    os.remove(video_path)\n",
        "                    print(f\"  üóëÔ∏è Removed video: {video_path}\")\n",
        "                    removed_count += 1\n",
        "                except Exception as e:\n",
        "                    print(f\"  ‚ùå Error removing video {video_path}: {e}\")\n",
        "            else:\n",
        "                print(f\"  ü§∑ Video file not found: {video_path}\")\n",
        "        else:\n",
        "            print(f\"  ‚ÑπÔ∏è No valid video filename listed.\")\n",
        "\n",
        "        # 2. Remove Audio File\n",
        "        if pd.notna(audio_name) and audio_name not in [\"FAILED\", \"SKIPPED_DURATION\"]:\n",
        "            audio_path = os.path.join(audio_dir, audio_name)\n",
        "            if os.path.exists(audio_path):\n",
        "                try:\n",
        "                    os.remove(audio_path)\n",
        "                    print(f\"  üóëÔ∏è Removed audio: {audio_path}\")\n",
        "                    removed_count += 1\n",
        "                except Exception as e:\n",
        "                    print(f\"  ‚ùå Error removing audio {audio_path}: {e}\")\n",
        "            else:\n",
        "                print(f\"  ü§∑ Audio file not found: {audio_path}\")\n",
        "        else:\n",
        "            print(f\"  ‚ÑπÔ∏è No valid audio filename listed.\")\n",
        "\n",
        "    # 3. Update the metadata CSV file\n",
        "    try:\n",
        "        # Drop the temporary column before saving\n",
        "        df_to_keep = df_to_keep.drop(columns=['duration_numeric'])\n",
        "        df_to_keep.to_csv(metadata_path, index=False)\n",
        "        print(f\"\\n‚úÖ Successfully updated metadata file: {metadata_path}\")\n",
        "        print(f\"Removed {len(df_to_remove)} entries from CSV.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå CRITICAL: Error writing updated metadata file: {e}\")\n",
        "        print(\"   Your files may be deleted, but the CSV was not updated.\")\n",
        "\n",
        "    print(f\"\\n--- Cleanup Summary ---\")\n",
        "    print(f\"Removed {len(df_to_remove)} videos from metadata.\")\n",
        "    print(f\"Deleted {removed_count} associated files.\")\n",
        "    print(\"--- Cleanup Finished ---\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Updated main block\n",
        "    print(f\"--- Video Cleanup Utility ---\")\n",
        "    print(f\"This script will find files over {MAX_DURATION_SECONDS} seconds.\")\n",
        "    print(f\"It will read from: {METADATA_FILE}\")\n",
        "    print(f\"It will look for files in: {VIDEOS_DIRECTORY} and {AUDIO_DIRECTORY}\")\n",
        "    print(\"You will be asked for confirmation before any files are deleted.\")\n",
        "\n",
        "    # Check if a command-line argument is provided to auto-confirm\n",
        "    if len(sys.argv) > 1 and sys.argv[1].lower() == '--yes':\n",
        "        print(\"\\n'--yes' flag detected, auto-confirming...\")\n",
        "        # This part is for automation, but the main logic is now inside the function\n",
        "        # We'll just call the function, but the function itself will now ask.\n",
        "        # Let's adjust the logic. The user *probably* wants --yes to bypass the *new* check.\n",
        "\n",
        "        # Let's re-think the main block logic to better support --yes\n",
        "\n",
        "        # We need to pass the confirmation status *into* the function.\n",
        "        # I will refactor.\n",
        "        pass # Will rewrite the main block and function slightly.\n",
        "\n",
        "\n",
        "# --- Let's refactor the code to handle the confirmation logic better ---\n",
        "\n",
        "def find_videos_to_remove(metadata_path, max_seconds):\n",
        "    \"\"\"Finds videos over the duration without deleting.\"\"\"\n",
        "    if not os.path.exists(metadata_path):\n",
        "        print(f\"‚ùå Error: Metadata file not found at '{metadata_path}'. Cannot proceed.\")\n",
        "        return None, None\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(metadata_path)\n",
        "    except pd.errors.EmptyDataError:\n",
        "        print(\"‚úÖ Metadata file is empty. Nothing to clean up.\")\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error reading metadata file: {e}\")\n",
        "        return None, None\n",
        "\n",
        "    if 'duration_seconds' not in df.columns:\n",
        "        print(\"‚ùå Error: 'duration_seconds' column not found in metadata.\")\n",
        "        return None, None\n",
        "\n",
        "    df['duration_numeric'] = pd.to_numeric(df['duration_seconds'], errors='coerce')\n",
        "    to_keep_mask = df['duration_numeric'].fillna(0) <= max_seconds\n",
        "\n",
        "    df_to_keep = df[to_keep_mask]\n",
        "    df_to_remove = df[~to_keep_mask]\n",
        "\n",
        "    return df_to_keep, df_to_remove\n",
        "\n",
        "\n",
        "def delete_videos(df_to_remove, df_to_keep, metadata_path, videos_dir, audio_dir):\n",
        "    \"\"\"Performs the actual deletion of files and updates the CSV.\"\"\"\n",
        "\n",
        "    print(\"\\nProceeding with deletion...\")\n",
        "    removed_count = 0\n",
        "\n",
        "    for index, row in df_to_remove.iterrows():\n",
        "        video_name = row.get('filename')\n",
        "        audio_name = row.get('audio_filename')\n",
        "        title = row.get('title', f\"URL: {row.get('url', 'N/A')}\")\n",
        "\n",
        "        print(f\"\\nProcessing '{title}' (Duration: {row.get('duration_seconds')}s)\")\n",
        "\n",
        "        # 1. Remove Video File\n",
        "        if pd.notna(video_name) and video_name not in [\"FAILED\", \"SKIPPED_DURATION\"]:\n",
        "            video_path = os.path.join(videos_dir, video_name)\n",
        "            if os.path.exists(video_path):\n",
        "                try:\n",
        "                    os.remove(video_path)\n",
        "                    print(f\"  üóëÔ∏è Removed video: {video_path}\")\n",
        "                    removed_count += 1\n",
        "                except Exception as e:\n",
        "                    print(f\"  ‚ùå Error removing video {video_path}: {e}\")\n",
        "            else:\n",
        "                print(f\"  ü§∑ Video file not found: {video_path}\")\n",
        "        else:\n",
        "            print(f\"  ‚ÑπÔ∏è No valid video filename listed.\")\n",
        "\n",
        "        # 2. Remove Audio File\n",
        "        if pd.notna(audio_name) and audio_name not in [\"FAILED\", \"SKIPPED_DURATION\"]:\n",
        "            audio_path = os.path.join(audio_dir, audio_name)\n",
        "            if os.path.exists(audio_path):\n",
        "                try:\n",
        "                    os.remove(audio_path)\n",
        "                    print(f\"  üóëÔ∏è Removed audio: {audio_path}\")\n",
        "                    removed_count += 1\n",
        "                except Exception as e:\n",
        "                    print(f\"  ‚ùå Error removing audio {audio_path}: {e}\")\n",
        "            else:\n",
        "                print(f\"  ü§∑ Audio file not found: {audio_path}\")\n",
        "        else:\n",
        "            print(f\"  ‚ÑπÔ∏è No valid audio filename listed.\")\n",
        "\n",
        "    # 3. Update the metadata CSV file\n",
        "    try:\n",
        "        # Drop the temporary column before saving\n",
        "        if 'duration_numeric' in df_to_keep.columns:\n",
        "            df_to_keep = df_to_keep.drop(columns=['duration_numeric'])\n",
        "\n",
        "        df_to_keep.to_csv(metadata_path, index=False)\n",
        "        print(f\"\\n‚úÖ Successfully updated metadata file: {metadata_path}\")\n",
        "        print(f\"Removed {len(df_to_remove)} entries from CSV.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå CRITICAL: Error writing updated metadata file: {e}\")\n",
        "        print(\"   Your files may be deleted, but the CSV was not updated.\")\n",
        "\n",
        "    print(f\"\\n--- Cleanup Summary ---\")\n",
        "    print(f\"Removed {len(df_to_remove)} videos from metadata.\")\n",
        "    print(f\"Deleted {removed_count} associated files.\")\n",
        "    print(\"--- Cleanup Finished ---\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print(f\"--- Video Cleanup Utility ---\")\n",
        "    print(f\"This script will find files over {MAX_DURATION_SECONDS} seconds.\")\n",
        "    print(f\"It will read from: {METADATA_FILE}\")\n",
        "    print(f\"It will look for files in: {VIDEOS_DIRECTORY} and {AUDIO_DIRECTORY}\")\n",
        "\n",
        "    # 1. Find videos\n",
        "    df_to_keep, df_to_remove = find_videos_to_remove(METADATA_FILE, MAX_DURATION_SECONDS)\n",
        "\n",
        "    # 2. Check results\n",
        "    if df_to_remove is None or df_to_remove.empty:\n",
        "        if df_to_remove is not None: # This means it was empty, not an error\n",
        "             print(f\"‚úÖ No videos found exceeding the {MAX_DURATION_SECONDS}s threshold.\")\n",
        "             # We might need to save the DF to remove the temp column\n",
        "             if df_to_keep is not None and 'duration_numeric' in df_to_keep.columns:\n",
        "                 df_to_keep = df_to_keep.drop(columns=['duration_numeric'])\n",
        "                 df_to_keep.to_csv(METADATA_FILE, index=False)\n",
        "        sys.exit() # Exit script\n",
        "\n",
        "    # 3. List videos\n",
        "    print(f\"\\nFound {len(df_to_remove)} video(s) to remove (duration > {MAX_DURATION_SECONDS}s):\")\n",
        "    print(\"------------------------------\")\n",
        "    for index, row in df_to_remove.iterrows():\n",
        "        title = row.get('title', f\"URL: {row.get('url', 'N/A')}\")\n",
        "        duration = row.get('duration_seconds', 'N/A')\n",
        "        print(f\"  - {title} (Duration: {duration}s)\")\n",
        "    print(\"------------------------------\")\n",
        "\n",
        "    # 4. Check for auto-confirmation or ask user\n",
        "    auto_confirm = len(sys.argv) > 1 and sys.argv[1].lower() == '--yes'\n",
        "\n",
        "    if auto_confirm:\n",
        "        print(\"\\n'--yes' flag detected, proceeding with deletion...\")\n",
        "        proceed = True\n",
        "    else:\n",
        "        confirm = input(\"\\nAre you sure you want to proceed with deleting these files and entries? (yes/no): \")\n",
        "        proceed = confirm.lower() == 'yes'\n",
        "\n",
        "    # 5. Execute deletion if confirmed\n",
        "    if proceed:\n",
        "        delete_videos(df_to_remove, df_to_keep, METADATA_FILE, VIDEOS_DIRECTORY, AUDIO_DIRECTORY)\n",
        "    else:\n",
        "        print(\"Operation cancelled by user.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNMCmgSA5NsJ",
        "outputId": "d9d217e9-1e52-4c54-dec3-60b81928b132"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Video Cleanup Utility ---\n",
            "This script will find files over 1500 seconds.\n",
            "It will read from: ./videos/video_metadata.csv\n",
            "It will look for files in: ./videos and ./audio\n",
            "You will be asked for confirmation before any files are deleted.\n",
            "--- Video Cleanup Utility ---\n",
            "This script will find files over 1500 seconds.\n",
            "It will read from: ./videos/video_metadata.csv\n",
            "It will look for files in: ./videos and ./audio\n",
            "\n",
            "Found 1 video(s) to remove (duration > 1500s):\n",
            "------------------------------\n",
            "  - Understanding cataract and lens surgery.  How we explain it.  Shannon Wong, MD (Duration: 1643s)\n",
            "------------------------------\n",
            "\n",
            "Are you sure you want to proceed with deleting these files and entries? (yes/no): no\n",
            "Operation cancelled by user.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"CUDA memory cleared.\")\n",
        "else:\n",
        "    print(\"CUDA not available.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsjGuu_d1byJ",
        "outputId": "745a9e0b-42ee-4b35-b40f-033982f8ed5b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA memory cleared.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/openai/whisper.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-h7V_Cb2Jom",
        "outputId": "d1caa6e5-038c-41fa-ea5f-536b9cd0d69b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcLT3SUT2OOA",
        "outputId": "e24edc14-213b-4c1c-91e8-5334fba9d349"
      },
      "outputs": [],
=======
>>>>>>> parent of c980829 (Added Refinement of Transcripts Using Cerebras LLM)
      "source": [
        "# pip install git+https://github.com/openai/whisper.git\n",
        "\n",
        "import os\n",
        "import whisper\n",
        "import torch\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "def transcribe_audio_files(input_dir: str = './audio', output_dir: str = './transcripts'):\n",
        "    \"\"\"\n",
        "    Transcribes all .wav files in the input directory using Whisper's large-v3 model,\n",
        "    capturing sentence-level timestamps, and saves the output as .json files.\n",
        "\n",
        "      Args:\n",
        "        input_dir: The directory containing the .wav files (16kHz mono).\n",
        "        output_dir: The directory where the transcription .json files will be saved.\n",
        "    \"\"\"\n",
        "    print(\"--- Starting Audio Transcription Process ---\")\n",
        "\n",
        "    # 1. Setup directories and check for GPU\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f\"Using device: {device.upper()}\")\n",
        "    if device == 'cpu':\n",
        "        print(\"‚ö†Ô∏è WARNING: No GPU found. Transcription will be very slow.\")\n",
        "\n",
        "    # 2. Load the pre-trained Whisper model\n",
        "    print(\"Loading Whisper model (large)...\")\n",
        "    try:\n",
        "        model = whisper.load_model(\"large\", device=device)\n",
        "        print(\"‚úÖ Model loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading Whisper model: {e}\")\n",
        "        print(\"Please check your internet connection and if 'openai-whisper' is installed correctly.\")\n",
        "        return\n",
        "\n",
        "    # 3. Identify audio files to process\n",
        "    audio_files = {os.path.splitext(f)[0] for f in os.listdir(input_dir) if f.endswith('.wav')}\n",
        "    transcribed_files = {os.path.splitext(f)[0] for f in os.listdir(output_dir) if f.endswith('.json')}\n",
        "    files_to_process = sorted([f + '.wav' for f in (audio_files - transcribed_files)])\n",
        "\n",
        "    if not files_to_process:\n",
        "        print(\"‚úÖ All audio files have already been transcribed.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(files_to_process)} audio file(s) to transcribe.\")\n",
        "\n",
        "    # 4. Process each audio file\n",
        "    for filename in tqdm(files_to_process, desc=\"Transcribing Audio\"):\n",
        "        input_path = os.path.join(input_dir, filename)\n",
        "        output_filename = f\"{os.path.splitext(filename)[0]}.json\"\n",
        "        output_path = os.path.join(output_dir, output_filename)\n",
        "\n",
        "        try:\n",
        "            # Perform transcription (sentence/segment-level timestamps by default)\n",
        "            result = model.transcribe(input_path, fp16=torch.cuda.is_available())\n",
        "\n",
        "            # Extract sentence-level segments and round timestamps to one decimal place\n",
        "            segments = [\n",
        "                {\n",
        "                    \"start\": round(seg[\"start\"], 1),\n",
        "                    \"end\": round(seg[\"end\"], 1),\n",
        "                    \"text\": seg[\"text\"].strip()\n",
        "                }\n",
        "                for seg in result[\"segments\"]\n",
        "            ]\n",
        "\n",
        "            # Save just the clean sentence-level transcript with timestamps\n",
        "            with open(output_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(segments, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "        except Exception as e:\n",
        "            tqdm.write(f\"‚ùå Error transcribing {filename}: {e}\")\n",
        "\n",
        "    print(\"\\n--- Audio Transcription process completed. ---\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Make sure ffmpeg is installed on your system and accessible in your PATH.\n",
        "    # On Debian /Ubuntu: sudo apt update && sudo apt install ffmpeg\n",
        "    # On macOS (using Homebrew): brew install ffmpeg\n",
        "    # On Windows (using Chocolatey): choco install ffmpeg\n",
        "    transcribe_audio_files()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcLT3SUT2OOA",
        "outputId": "e24edc14-213b-4c1c-91e8-5334fba9d349"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Audio Transcription Process ---\n",
            "Using device: CUDA\n",
            "Loading Whisper model (large)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.88G/2.88G [00:39<00:00, 78.1MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model loaded successfully.\n",
            "Found 65 audio file(s) to transcribe.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Transcribing Audio:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 46/65 [1:39:09<50:51, 160.60s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transcript Refinement\n",
        "\n",
        "This script refines existing JSON transcripts using a Cerebras LLM\n",
        "and formats the corrected output into a human-readable .txt file\n",
        "with [MM:SS - MM:SS] timestamps.\n",
        "\n",
        "It saves two files for each transcript:\n",
        "1.  ./refined_transcripts/BASENAME.txt - The final formatted text.\n",
        "2.  ./refined_transcripts/full_responses/BASENAME_full_response.txt - The raw LLM output."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lTnnINX649fh"
      },
<<<<<<< HEAD
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Transcript Refinement Pipeline (to TXT)\n",
        "\n",
        "This script refines existing JSON transcripts using a Cerebras LLM\n",
        "and formats the corrected output into a human-readable .txt file\n",
        "with [MM:SS - MM:SS] timestamps.\n",
        "\n",
        "It saves two files for each transcript:\n",
        "1.  ./refined_transcripts/BASENAME.txt - The final formatted text.\n",
        "2.  ./refined_transcripts/full_responses/BASENAME_full_response.txt - The raw LLM output.\n",
        "\n",
        "It also checks for a corresponding video in './videos' and logs\n",
        "metadata to './refined_transcripts/refinement_log.csv'.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import math # Needed for floor/ceiling or int casting\n",
        "import glob # To find video files with any extension\n",
        "import csv  # To write the log file\n",
        "\n",
        "# --- Imports for Refinement ---\n",
        "try:\n",
        "    # Used for secure API key loading in Colab\n",
        "    from google.colab import userdata\n",
        "    COLAB_ENV = True\n",
        "except ImportError:\n",
        "    # Fallback for local development\n",
        "    from dotenv import load_dotenv\n",
        "    COLAB_ENV = False\n",
        "\n",
        "# Direct library imports as requested\n",
        "from cerebras.cloud.sdk import Cerebras\n",
        "import cv2\n",
        "\n",
        "\n",
        "# --- General Configuration ---\n",
        "# Directory containing the raw .json transcripts from Whisper\n",
        "INPUT_DIR = './transcripts'\n",
        "# NEW: Directory containing the source video files\n",
        "VIDEOS_DIR = './videos'\n",
        "# Main output directory for the final .txt files\n",
        "REFINED_OUTPUT_DIR = './refined_transcripts'\n",
        "# Sub-directory for all raw LLM responses\n",
        "FULL_RESPONSE_DIR = os.path.join(REFINED_OUTPUT_DIR, 'full_responses')\n",
        "# NEW: Path for the CSV log file\n",
        "CSV_LOG_PATH = os.path.join(REFINED_OUTPUT_DIR, 'refinement_log.csv')\n",
        "\n",
        "\n",
        "# --- API Rate Limiting Configuration ---\n",
        "API_CALL_DELAY_SECONDS = 45 # Time to wait between each file (in seconds)\n",
        "MAX_FILES_PER_RUN = 100     # Maximum number of transcripts to process in one run\n",
        "\n",
        "\n",
        "# --- LLM and Prompt Configuration ---\n",
        "MODEL_NAME = \"qwen-3-235b-a22b-thinking-2507\"\n",
        "\n",
        "MEDICAL_EDITOR_SYSTEM_PROMPT = \"\"\"You are an expert JSON and medical editor. Your task is to correct typos, punctuation, and grammatical errors in a JSON file provided by the user, while preserving its exact structure.\n",
        "\n",
        "The user will provide a JSON array of segments from a cataract surgery video.\n",
        "Your job is to fix errors **only** in the \"text\" fields.\n",
        "\n",
        "**CRITICAL INSTRUCTIONS:**\n",
        "1.  Read the user's JSON, perform your corrections, and think.\n",
        "2.  You **MUST** return the JSON in the **EXACT** same array format, including \"start\", \"end\", and \"text\" keys for every segment.\n",
        "3.  **DO NOT** alter the \"start\", \"end\", or any other part of the JSON structure.\n",
        "4.  **DO NOT** include any commentary, conversational replies, or pre-amble (like \"Here is the corrected JSON:\").\n",
        "5.  The output must be the pure, corrected JSON data and nothing else.\"\"\"\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# HELPER FUNCTIONS\n",
        "# ==============================================================================\n",
        "\n",
        "def get_api_key():\n",
        "    \"\"\"Gets the Cerebras API key from Colab secrets or environment variables.\"\"\"\n",
        "    if COLAB_ENV:\n",
        "        try:\n",
        "            key = userdata.get('CEREBRAS_API_KEY')\n",
        "            if key is None:\n",
        "                print(\"CEREBRAS_API_KEY not found in Colab userdata.\")\n",
        "                return None\n",
        "            return key\n",
        "        except Exception as e:\n",
        "            print(f\"Error accessing Colab userdata: {e}\")\n",
        "            return None\n",
        "    else:\n",
        "        return os.environ.get(\"CEREBRAS_API_KEY\")\n",
        "\n",
        "def parse_llm_json_output(raw_output: str) -> str:\n",
        "    \"\"\"\n",
        "    Cleans the output from a 'thinking' model.\n",
        "    Locates the final </think> tag and returns the JSON array *after* it.\n",
        "    \"\"\"\n",
        "    # Use the correct tag as you specified\n",
        "    THINK_END_TAG = \"</think>\"\n",
        "    last_tag_index = raw_output.rfind(THINK_END_TAG)\n",
        "    \n",
        "    if last_tag_index != -1:\n",
        "        # Found the tag, take text after it\n",
        "        cleaned_text = raw_output[last_tag_index + len(THINK_END_TAG):]\n",
        "    else:\n",
        "        # No tag found, assume the entire output is the answer\n",
        "        cleaned_text = raw_output\n",
        "\n",
        "    # --- ROBUST JSON EXTRACTION ---\n",
        "    # 1. Strip all leading/trailing whitespace\n",
        "    stripped_text = cleaned_text.strip()\n",
        "\n",
        "    # 2. Find the first '[' and last ']'\n",
        "    start_brace = stripped_text.find('[')\n",
        "    end_brace = stripped_text.rfind(']')\n",
        "\n",
        "    if start_brace != -1 and end_brace != -1 and end_brace > start_brace:\n",
        "        # Found a [..] block. Extract it.\n",
        "        json_only_text = stripped_text[start_brace : end_brace + 1]\n",
        "        return json_only_text\n",
        "    else:\n",
        "        # Could not find a [..] block.\n",
        "        return stripped_text # This will fail json.loads() and be logged\n",
        "\n",
        "def format_time_to_mm_ss(seconds: float) -> str:\n",
        "    \"\"\"Converts a float number of seconds to MM:SS format.\"\"\"\n",
        "    seconds_int = int(seconds)\n",
        "    minutes = seconds_int // 60\n",
        "    seconds_rem = seconds_int % 60\n",
        "    # Formats as \"02:05\"\n",
        "    return f\"{minutes:02d}:{seconds_rem:02d}\"\n",
        "\n",
        "def format_segments_to_txt(segments: list) -> str:\n",
        "    \"\"\"\n",
        "    Converts a list of segment objects into a single, formatted\n",
        "    string with [MM:SS - MM:SS]: text format.\n",
        "    \"\"\"\n",
        "    formatted_lines = []\n",
        "    for seg in segments:\n",
        "        try:\n",
        "            start_time = format_time_to_mm_ss(seg[\"start\"])\n",
        "            end_time = format_time_to_mm_ss(seg[\"end\"])\n",
        "            text = seg[\"text\"].strip()\n",
        "            \n",
        "            formatted_lines.append(f\"[{start_time} - {end_time}]: {text}\")\n",
        "        except KeyError:\n",
        "            # Skip a segment if it's malformed (missing start/end/text)\n",
        "            tqdm.write(\"  - Warning: Skipping malformed segment.\")\n",
        "            continue\n",
        "    \n",
        "    return \"\\n\".join(formatted_lines)\n",
        "\n",
        "def get_video_details(videos_dir: str, basename: str) -> tuple[str | None, float | None]:\n",
        "    \"\"\"\n",
        "    Finds a video matching the basename and returns its path and duration.\n",
        "    Searches for common video extensions.\n",
        "    \"\"\"\n",
        "    # Removed cv2 check\n",
        "        \n",
        "    # Use glob to find any matching video file (mp4, avi, mov, etc.)\n",
        "    video_search_pattern = os.path.join(videos_dir, f\"{basename}.*\")\n",
        "    video_files = glob.glob(video_search_pattern)\n",
        "    \n",
        "    # Filter for common video extensions (glob might pick up .txt, .json, etc.)\n",
        "    common_extensions = ['.mp4', '.mov', '.avi', '.mkv', '.webm', '.flv']\n",
        "    found_video_path = None\n",
        "    for f in video_files:\n",
        "        if os.path.splitext(f)[1].lower() in common_extensions:\n",
        "            found_video_path = f\n",
        "            break\n",
        "            \n",
        "    if not found_video_path:\n",
        "        return None, None\n",
        "\n",
        "    try:\n",
        "        cap = cv2.VideoCapture(found_video_path)\n",
        "        if not cap.isOpened():\n",
        "            tqdm.write(f\"  - Warning: Could not open video file {found_video_path}\")\n",
        "            return None, None\n",
        "            \n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        \n",
        "        if fps > 0 and frame_count > 0:\n",
        "            duration = frame_count / fps\n",
        "        else:\n",
        "            tqdm.write(f\"  - Warning: Could not read metadata from {found_video_path}\")\n",
        "            duration = 0.0 # Unknown duration\n",
        "            \n",
        "        cap.release()\n",
        "        return found_video_path, round(duration, 2)\n",
        "        \n",
        "    except Exception as e:\n",
        "        tqdm.write(f\"‚ùå Error processing video {found_video_path}: {e}\")\n",
        "        if 'cap' in locals() and cap.isOpened():\n",
        "            cap.release()\n",
        "        return None, None\n",
        "\n",
        "def refine_text_chunk_with_llm(text: str, model_name: str, api_key: str) -> str | None:\n",
        "    \"\"\"\n",
        "    Sends a block of text to the Cerebras model for correction.\n",
        "    Returns the FULL RAW response from the LLM.\n",
        "    \"\"\"\n",
        "    # Removed Cerebras check\n",
        "\n",
        "    try:\n",
        "        client = Cerebras(api_key=api_key)\n",
        "        \n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": MEDICAL_EDITOR_SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": text}\n",
        "        ]\n",
        "\n",
        "        chat_completion = client.chat.completions.create(\n",
        "            messages=messages,\n",
        "            model=model_name,\n",
        "        )\n",
        "        \n",
        "        # Return the entire raw output for logging\n",
        "        raw_output = chat_completion.choices[0].message.content\n",
        "        return raw_output\n",
        "    \n",
        "    except Exception as e:\n",
        "        if \"context\" in str(e).lower() or \"limit\" in str(e).lower() or \"too large\" in str(e).lower():\n",
        "             tqdm.write(f\"‚ùå CRITICAL: The transcript JSON is too long for the model. Error: {e}\")\n",
        "             return '{\"error\": \"--- REFINEMENT FAILED: MODEL CONTEXT LIMIT EXCEEDED ---\"}'\n",
        "        \n",
        "        tqdm.write(f\"‚ùå An error occurred during Cerebras API call: {e}\")\n",
        "        tqdm.write(\"    Will retry after 10 seconds.\")\n",
        "        time.sleep(10) # Wait 10s on API error\n",
        "        return None # Signal a failure to be retried\n",
        "\n",
        "# ==============================================================================\n",
        "# MAIN REFINEMENT FUNCTION\n",
        "# ==============================================================================\n",
        "\n",
        "def setup_csv_log(csv_path: str):\n",
        "    \"\"\"Creates the CSV log file with a header if it doesn't exist.\"\"\"\n",
        "    # Check if file already exists\n",
        "    if os.path.exists(csv_path):\n",
        "        return # Header already exists\n",
        "        \n",
        "    try:\n",
        "        with open(csv_path, mode='w', newline='', encoding='utf-8') as f:\n",
        "            writer = csv.writer(f)\n",
        "            # Write the header row\n",
        "            writer.writerow([\n",
        "                \"video_name\", \n",
        "                \"transcript_name\", \n",
        "                \"duration (seconds)\", \n",
        "                \"total_characters\",\n",
        "                \"total_words\"  # <-- MODIFIED: Added total_words column\n",
        "            ])\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå CRITICAL: Could not create CSV log file at {csv_path}. Error: {e}\")\n",
        "\n",
        "def append_to_csv_log(csv_path: str, data_row: list):\n",
        "    \"\"\"Appends a single row of data to the CSV log file.\"\"\"\n",
        "    try:\n",
        "        with open(csv_path, mode='a', newline='', encoding='utf-8') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow(data_row)\n",
        "    except Exception as e:\n",
        "        tqdm.write(f\"‚ùå Warning: Could not write to CSV log file. Error: {e}\")\n",
        "\n",
        "def refine_transcripts(\n",
        "    input_dir: str, \n",
        "    videos_dir: str, \n",
        "    output_dir: str, \n",
        "    full_response_dir: str, \n",
        "    csv_log_path: str,\n",
        "    model_name: str\n",
        "):\n",
        "    \"\"\"\n",
        "    Finds and processes raw transcripts, refining them with an LLM and saving\n",
        "    to .txt and a full response log.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Starting Transcript Refinement (to .txt) ---\")\n",
        "    \n",
        "    # Create both output directories\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    os.makedirs(full_response_dir, exist_ok=True)\n",
        "    \n",
        "    # --- NEW: Setup CSV Log File ---\n",
        "    setup_csv_log(csv_log_path)\n",
        "    # ---\n",
        "\n",
        "    api_key = get_api_key()\n",
        "    if not api_key:\n",
        "        print(\"‚ùå Error: CEREBRAS_API_KEY not found.\")\n",
        "        print(\"Please set it in Colab secrets or a local .env file.\")\n",
        "        print(\"Aborting refinement script.\")\n",
        "        return\n",
        "    print(\"‚úÖ CEREBRAS_API_KEY loaded successfully.\")\n",
        "    \n",
        "    # Removed cv2 check\n",
        "\n",
        "    try:\n",
        "        raw_transcripts = {os.path.splitext(f)[0] for f in os.listdir(input_dir) if f.endswith('.json')}\n",
        "        # Check for .txt files in the *main* output directory\n",
        "        refined_transcripts = {os.path.splitext(f)[0] for f in os.listdir(output_dir) if f.endswith('.txt')}\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"‚ùå Error: Directory not found: {e.filename}\")\n",
        "        print(\"Please ensure all directories (input, videos, output) are correct.\")\n",
        "        return\n",
        "        \n",
        "    files_to_process = sorted([f for f in (raw_transcripts - refined_transcripts)])\n",
        "\n",
        "    if not files_to_process:\n",
        "        print(\"‚úÖ All transcripts have already been refined.\")\n",
        "        print(\"--- Refinement Finished ---\")\n",
        "        return\n",
        "\n",
        "    # --- NEW: Apply MAX_FILES_PER_RUN limit ---\n",
        "    original_count = len(files_to_process)\n",
        "    files_to_process = files_to_process[:MAX_FILES_PER_RUN]\n",
        "    processed_count = len(files_to_process)\n",
        "\n",
        "    print(f\"Found {original_count} total unprocessed transcript(s).\")\n",
        "    print(f\"--> Limiting this run to {processed_count} file(s) (MAX_FILES_PER_RUN = {MAX_FILES_PER_RUN}).\")\n",
        "    # ---\n",
        "\n",
        "    for basename in tqdm(files_to_process, desc=\"Refining Files\"):\n",
        "        \n",
        "        # --- NEW: Video Validation Step ---\n",
        "        video_path, video_duration = get_video_details(videos_dir, basename)\n",
        "        \n",
        "        if video_path is None:\n",
        "            tqdm.write(f\"  - Skipping {basename}: No matching video file found in {videos_dir}\")\n",
        "            continue\n",
        "        \n",
        "        tqdm.write(f\"  + Found video: {os.path.basename(video_path)} ({video_duration}s)\")\n",
        "        # ---\n",
        "        \n",
        "        transcript_path = os.path.join(input_dir, f\"{basename}.json\")\n",
        "        \n",
        "        # Define our two new output paths\n",
        "        txt_output_path = os.path.join(output_dir, f\"{basename}.txt\")\n",
        "        full_response_path = os.path.join(full_response_dir, f\"{basename}_full_response.txt\")\n",
        "\n",
        "        try:\n",
        "            with open(transcript_path, 'r', encoding='utf-8') as f:\n",
        "                segments = json.load(f)\n",
        "        except Exception as e:\n",
        "            tqdm.write(f\"‚ùå Error loading {basename}.json: {e}\")\n",
        "            continue\n",
        "\n",
        "        if not segments:\n",
        "            tqdm.write(f\"‚ÑπÔ∏è Skipping {basename}: No content to process.\")\n",
        "            continue\n",
        "            \n",
        "        try:\n",
        "            full_json_text = json.dumps(segments, indent=4)\n",
        "        except Exception as e:\n",
        "            tqdm.write(f\"‚ùå Error serializing JSON for {basename}: {e}\")\n",
        "            continue\n",
        "\n",
        "        tqdm.write(f\"  - Refining {basename} ({len(full_json_text)} chars)...\")\n",
        "        raw_llm_response = None\n",
        "        while raw_llm_response is None: # Loop until successful\n",
        "            raw_llm_response = refine_text_chunk_with_llm(full_json_text, model_name, api_key)\n",
        "            if raw_llm_response is None:\n",
        "                tqdm.write(f\"    Retrying full transcript for {basename}...\")\n",
        "\n",
        "        # 1. Save the full raw response for debugging\n",
        "        try:\n",
        "            with open(full_response_path, 'w', encoding='utf-8') as f:\n",
        "                f.write(raw_llm_response)\n",
        "        except Exception as e:\n",
        "            tqdm.write(f\"‚ùå Error saving full response log for {basename}: {e}\")\n",
        "\n",
        "        # 2. Try to parse and save the formatted .txt file\n",
        "        try:\n",
        "            # Get just the [..] part from the raw response\n",
        "            json_string = parse_llm_json_output(raw_llm_response)\n",
        "            \n",
        "            # Convert string to Python list\n",
        "            segments_list = json.loads(json_string)\n",
        "\n",
        "            # --- MODIFIED: Calculate total_words from segments list ---\n",
        "            total_words = 0\n",
        "            for seg in segments_list:\n",
        "                # Use .get() for safety, defaulting to empty string\n",
        "                text = seg.get(\"text\", \"\") \n",
        "                total_words += len(text.split())\n",
        "            # --- END MODIFICATION ---\n",
        "\n",
        "            # Format the list into the [MM:SS - MM:SS] text\n",
        "            formatted_txt_content = format_segments_to_txt(segments_list)\n",
        "            \n",
        "            # Save the final .txt file\n",
        "            with open(txt_output_path, 'w', encoding='utf-8') as f:\n",
        "                f.write(formatted_txt_content)\n",
        "                \n",
        "            # --- NEW: Log to CSV ---\n",
        "            total_chars = len(formatted_txt_content) # This includes timestamps\n",
        "            video_name = os.path.basename(video_path)\n",
        "            transcript_name = os.path.basename(txt_output_path)\n",
        "            \n",
        "            # MODIFIED: Added total_words\n",
        "            log_data = [video_name, transcript_name, video_duration, total_chars, total_words]\n",
        "            append_to_csv_log(csv_log_path, log_data)\n",
        "            # ---\n",
        "                \n",
        "        except json.JSONDecodeError:\n",
        "            tqdm.write(f\"‚ùå CRITICAL: Could not parse valid JSON from LLM for {basename}.\")\n",
        "            tqdm.write(f\"    Check the log file: {full_response_path}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            tqdm.write(f\"‚ùå Error formatting or saving .txt for {basename}: {e}\")\n",
        "            tqdm.write(f\"    Check the log file: {full_response_path}\")\n",
        "            \n",
        "        # --- NEW: Add delay between API calls ---\n",
        "        # Don't sleep after the very last file in this batch\n",
        "        if basename != files_to_process[-1]:\n",
        "            try:\n",
        "                tqdm.write(f\"  - Pausing for {API_CALL_DELAY_SECONDS}s before next file...\")\n",
        "                time.sleep(API_CALL_DELAY_SECONDS)\n",
        "            except KeyboardInterrupt:\n",
        "                tqdm.write(\"\\n- Delay interrupted. Proceeding to next file...\")\n",
        "\n",
        "    print(\"\\n--- Refinement Process Completed ---\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ==============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Runs the full transcription and refinement pipeline.\n",
        "    \"\"\"\n",
        "    print(\"====== STARTING TRANSCRIPT REFINEMENT PIPELINE (TO .TXT) ======\")\n",
        "    \n",
        "    # Check for local .env file if not in Colab\n",
        "    if not COLAB_ENV:\n",
        "        print(\"Local environment detected. Loading .env file...\")\n",
        "        load_dotenv()\n",
        "    \n",
        "    # Removed library checks\n",
        "    refine_transcripts(\n",
        "        input_dir=INPUT_DIR,\n",
        "        videos_dir=VIDEOS_DIR,\n",
        "        output_dir=REFINED_OUTPUT_DIR,\n",
        "        full_response_dir=FULL_RESPONSE_DIR,\n",
        "        csv_log_path=CSV_LOG_PATH,\n",
        "        model_name=MODEL_NAME\n",
        "    )\n",
        "        \n",
        "    print(\"\\n====== TRANSCRIPT REFINEMENT FINISHED ======\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Removed pip install comments\n",
        "    main()"
      ]
=======
      "execution_count": null,
      "outputs": []
>>>>>>> parent of c980829 (Added Refinement of Transcripts Using Cerebras LLM)
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}