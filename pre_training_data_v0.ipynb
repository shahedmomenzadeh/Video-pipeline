{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9Iy2psGagD6"
      },
      "source": [
        "# Pre training video dataset creating from YouTube videos\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxRJdEmaafkh",
        "outputId": "feebe378-3926-4b9d-a404-53d55be6c05d"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/pre_training_phase_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffU-Juy2qcp-",
        "outputId": "07697498-1dbc-4bad-989a-fb302452bb69"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLibrFKFbx4N",
        "outputId": "943e5d41-d25e-4a50-e476-5f54fef10d60"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "videos_list = os.listdir(\"./videos\")\n",
        "print(f\"Number of Videos {len(videos_list)}\")\n",
        "print(f\"Number of Audios {len(os.listdir('./audio'))}\")\n",
        "print(f\"Number of transcripts {len(os.listdir('./transcripts'))}\")\n",
        "print(f\"Number of refined transcripts: {len(os.listdir('./refined_transcripts/'))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3awSaZHXZmiN"
      },
      "source": [
        "# Download YouTube Videos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ii0I2iJdZ2Of",
        "outputId": "f13d56f9-e408-4be3-b183-2175b7e65b38"
      },
      "outputs": [],
      "source": [
        "!pip install -q yt-dlp pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiKw1th1HfUY"
      },
      "outputs": [],
      "source": [
        "import yt_dlp\n",
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import shutil\n",
        "import subprocess\n",
        "import time  # Import the time module\n",
        "\n",
        "\n",
        "def is_ffmpeg_installed():\n",
        "    \"\"\"Check if FFmpeg is installed and available in the system's PATH.\"\"\"\n",
        "    return shutil.which(\"ffmpeg\") is not None\n",
        "\n",
        "\n",
        "def extract_audio_ffmpeg(video_filepath: str, audio_dir: str) -> str | None:\n",
        "    \"\"\"\n",
        "    Extracts audio from a video file using FFmpeg, converting it to 16kHz mono WAV.\n",
        "\n",
        "    Args:\n",
        "        video_filepath: The full path to the input video file.\n",
        "        audio_dir: The directory where the extracted audio will be saved.\n",
        "\n",
        "    Returns:\n",
        "        The filename of the extracted audio file if successful, otherwise None.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(video_filepath):\n",
        "        print(f\"‚ùå Error: Video file not found at {video_filepath}\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        video_basename = os.path.basename(video_filepath)\n",
        "        video_name_no_ext = os.path.splitext(video_basename)[0]\n",
        "        audio_filename = f\"{video_name_no_ext}.wav\"\n",
        "        output_audio_path = os.path.join(audio_dir, audio_filename)\n",
        "\n",
        "        print(f\"üéµ Extracting audio from '{video_basename}'...\")\n",
        "\n",
        "        # Command to extract audio, convert to PCM 16-bit little-endian,\n",
        "        # set sample rate to 16kHz, mono channel, and overwrite output\n",
        "        command = [\n",
        "            'ffmpeg', '-i', video_filepath, '-vn', '-acodec', 'pcm_s16le',\n",
        "            '-ar', '16000', '-ac', '1', '-y', output_audio_path\n",
        "        ]\n",
        "\n",
        "        # Run ffmpeg, suppressing stdout and stderr to keep the log clean\n",
        "        subprocess.run(command, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "        print(f\"‚úÖ Audio extracted: {output_audio_path}\")\n",
        "        return audio_filename\n",
        "\n",
        "    except subprocess.CalledProcessError:\n",
        "        print(f\"‚ùå FFmpeg error during audio extraction for {video_filepath}.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Unexpected error during audio extraction: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def download_video_and_extract_audio(video_url: str,\n",
        "                                     output_dir: str = './videos',\n",
        "                                     audio_dir: str = './audio',\n",
        "                                     metadata_file: str = './videos/video_metadata.csv',\n",
        "                                     cookie_file: str | None = None):\n",
        "    \"\"\"\n",
        "    Downloads a YouTube video, extracts its audio, logs metadata, and skips processed videos.\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    os.makedirs(audio_dir, exist_ok=True)\n",
        "\n",
        "    # Define the metadata columns\n",
        "    metadata_columns = [\n",
        "        'title', 'channel_name', 'url', 'filename',\n",
        "        'download_date', 'duration_seconds', 'resolution', 'audio_filename'\n",
        "    ]\n",
        "\n",
        "    # Load or initialize metadata DataFrame\n",
        "    if os.path.exists(metadata_file):\n",
        "        try:\n",
        "            metadata_df = pd.read_csv(metadata_file)\n",
        "            # Ensure all required columns exist (for backward compatibility)\n",
        "            for col in metadata_columns:\n",
        "                if col not in metadata_df.columns:\n",
        "                    metadata_df[col] = None\n",
        "            # Reorder columns for consistency\n",
        "            metadata_df = metadata_df[metadata_columns]\n",
        "        except pd.errors.EmptyDataError:\n",
        "            metadata_df = pd.DataFrame(columns=metadata_columns)\n",
        "    else:\n",
        "        metadata_df = pd.DataFrame(columns=metadata_columns)\n",
        "\n",
        "    # Skip if video URL already processed\n",
        "    if video_url in metadata_df['url'].values:\n",
        "        print(f\"‚è© Video already in metadata (skipped): {video_url}\")\n",
        "        return\n",
        "\n",
        "    # yt-dlp options\n",
        "    ydl_opts = {\n",
        "        # Get 480p video + best audio, merge into mp4\n",
        "        'format': 'bestvideo[height=480][ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best',\n",
        "        'outtmpl': os.path.join(output_dir, '%(title)s.%(ext)s'),\n",
        "        'noplaylist': True,\n",
        "        'merge_output_format': 'mp4',\n",
        "        'postprocessors': [{'key': 'FFmpegMetadata', 'add_chapters': False}],\n",
        "        'retries': 5,\n",
        "        'fragment_retries': 5,\n",
        "        'no_warnings': True, # Suppress warnings (like SABR)\n",
        "    }\n",
        "\n",
        "    if cookie_file:\n",
        "        ydl_opts['cookiefile'] = cookie_file\n",
        "\n",
        "    try:\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            # Extract info first without downloading\n",
        "            info = ydl.extract_info(video_url, download=False)\n",
        "            video_title = str(info.get('title', 'Unknown Title'))\n",
        "            channel_name = info.get('uploader', 'Unknown Channel')\n",
        "            duration = info.get('duration')\n",
        "            width, height = info.get('width'), info.get('height')\n",
        "            resolution = f\"{width}x{height}\" if width and height else \"N/A\"\n",
        "\n",
        "            # Get the expected downloaded video path\n",
        "            expected_video_path = ydl.prepare_filename(info)\n",
        "            video_name_no_ext = os.path.splitext(os.path.basename(expected_video_path))[0]\n",
        "            expected_audio_filename = f\"{video_name_no_ext}.wav\"\n",
        "            expected_audio_path = os.path.join(audio_dir, expected_audio_filename)\n",
        "\n",
        "            # Skip if audio file already exists\n",
        "            if os.path.exists(expected_audio_path):\n",
        "                print(f\"‚è© Audio already exists, assuming processed: {expected_audio_filename}\")\n",
        "                # Log metadata if it was missing (e.g., script interrupted)\n",
        "                if video_url not in metadata_df['url'].values:\n",
        "                    new_entry = pd.DataFrame([{\n",
        "                        'title': video_title,\n",
        "                        'channel_name': channel_name,\n",
        "                        'url': video_url,\n",
        "                        'filename': os.path.basename(expected_video_path),\n",
        "                        'download_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                        'duration_seconds': duration,\n",
        "                        'resolution': resolution,\n",
        "                        'audio_filename': expected_audio_filename\n",
        "                    }])\n",
        "                    metadata_df = pd.concat([metadata_df, new_entry], ignore_index=True)\n",
        "                    metadata_df.to_csv(metadata_file, index=False)\n",
        "                return\n",
        "\n",
        "            print(f\"‚¨áÔ∏è Downloading: '{video_title}' from channel: {channel_name}\")\n",
        "            ydl.download([video_url])\n",
        "\n",
        "            # Verify download and extract audio\n",
        "            if os.path.exists(expected_video_path):\n",
        "                print(f\"‚úÖ Download complete: {os.path.basename(expected_video_path)}\")\n",
        "                audio_filename = extract_audio_ffmpeg(expected_video_path, audio_dir)\n",
        "\n",
        "                # Log new entry to metadata\n",
        "                new_entry = pd.DataFrame([{\n",
        "                    'title': video_title,\n",
        "                    'channel_name': channel_name,\n",
        "                    'url': video_url,\n",
        "                    'filename': os.path.basename(expected_video_path),\n",
        "                    'download_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                    'duration_seconds': duration,\n",
        "                    'resolution': resolution,\n",
        "                    'audio_filename': audio_filename if audio_filename else \"N/A\"\n",
        "                }])\n",
        "                metadata_df = pd.concat([metadata_df, new_entry], ignore_index=True)\n",
        "                metadata_df.to_csv(metadata_file, index=False)\n",
        "            else:\n",
        "                print(f\"‚ùå Download reported success but file not found at '{expected_video_path}'\")\n",
        "\n",
        "    except yt_dlp.utils.DownloadError as e:\n",
        "        print(f\"‚ùå yt-dlp Download Error for {video_url}: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Unexpected error for {video_url}: {e}\")\n",
        "\n",
        "\n",
        "def verify_and_process_existing_videos(videos_dir: str, audio_dir: str):\n",
        "    \"\"\"\n",
        "    Scans the videos directory and extracts audio for any video missing its corresponding .wav file.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Verifying Existing Videos ---\")\n",
        "    if not os.path.isdir(videos_dir):\n",
        "        print(f\"‚ùå Verification skipped: '{videos_dir}' not found\")\n",
        "        return\n",
        "\n",
        "    # Get a set of audio filenames (without extension)\n",
        "    existing_audio_names = {os.path.splitext(f)[0] for f in os.listdir(audio_dir) if f.endswith('.wav')}\n",
        "    video_files = [f for f in os.listdir(videos_dir) if f.endswith(('.mp4', '.mkv', '.webm', '.mov'))]\n",
        "\n",
        "    # Find videos where the filename (without extension) is not in the audio set\n",
        "    missing_audio_videos = [\n",
        "        os.path.join(videos_dir, vf)\n",
        "        for vf in video_files\n",
        "        if os.path.splitext(vf)[0] not in existing_audio_names\n",
        "    ]\n",
        "\n",
        "    if not missing_audio_videos:\n",
        "        print(\"‚úÖ All videos have corresponding audio files.\")\n",
        "        return\n",
        "\n",
        "    print(f\"‚ö†Ô∏è Found {len(missing_audio_videos)} video(s) missing audio:\")\n",
        "    for v in missing_audio_videos:\n",
        "        print(f\"  - {os.path.basename(v)}\")\n",
        "\n",
        "    success, fail = 0, 0\n",
        "    for vpath in missing_audio_videos:\n",
        "        if extract_audio_ffmpeg(vpath, audio_dir):\n",
        "            success += 1\n",
        "        else:\n",
        "            fail += 1\n",
        "\n",
        "    print(\"\\n--- Verification Summary ---\")\n",
        "    print(f\"‚úÖ Extracted: {success}\")\n",
        "    print(f\"‚ùå Failed: {fail}\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Check for FFmpeg installation\n",
        "    if not is_ffmpeg_installed():\n",
        "        print(\"=\" * 60)\n",
        "        print(\"‚ö†Ô∏è FFmpeg is not installed or not in your system PATH.\")\n",
        "        print(\"   This script requires FFmpeg to extract audio.\")\n",
        "        print(\"   Download from: https://ffmpeg.org/download.html\")\n",
        "        print(\"=\" * 60)\n",
        "    else:\n",
        "        print(\"‚úÖ FFmpeg found.\")\n",
        "\n",
        "    # Define directories and files\n",
        "    VIDEOS_DIRECTORY = './videos'\n",
        "    AUDIO_DIRECTORY = './audio'\n",
        "    METADATA_FILE = os.path.join(VIDEOS_DIRECTORY, 'video_metadata.csv')\n",
        "\n",
        "    # List of videos to download\n",
        "    video_urls = [\n",
        "    # \"https://www.youtube.com/watch?v=OZ5SmpNFlU8&pp=ygUZY2F0YXJhY3Qgc3VyZ2VyeSBuYXJyYXRlZA%3D%3D\",\n",
        "    # \"https://youtube.com/playlist?list=PLjgfi4kp5BU7Lxk_O9GLCFzjXITHVonC0&si=ITWCQq8LzXfFRRKv\",\n",
        "    # \"https://youtube.com/playlist?list=PL88gdqtVPZep0oIpxOP1AStJPK0-Q2hEj&si=-NwuoIQ1VDi5M-Al\",\n",
        "    # \"https://youtube.com/playlist?list=PL88gdqtVPZeq0B1DAszU4uUmXbsksZsUf&si=vuURrNS4U10eE3hX\",\n",
        "    # \"https://youtube.com/playlist?list=PL88gdqtVPZepBwG8e9A4HMb-cy0DJmwvp&si=B6XLDmBGXNpYZ7GX\",\n",
        "    # \"https://www.youtube.com/playlist?list=PL88gdqtVPZerlilEVmKR-s3RTADHniLqu\",\n",
        "    # \"https://youtube.com/playlist?list=PLker0kXqgiOUV92h5wszstCLFQ-JFKATf&si=2OunF06tse7abgHt\",\n",
        "     \"https://youtube.com/playlist?list=PLker0kXqgiOVD5NNEES1UAYG7e-T6phV7&si=BQDJzsn_PHp_xFuK\",\n",
        "     \"https://youtu.be/-Q4uQ6rEExs?si=4mL-Ysen96rifmsb\",\n",
        "    \"https://youtu.be/LDhPCHvGxeA?si=xtkqrHo5gRzvUe4p\",\n",
        "    \"https://youtu.be/XjZ5r8GZq5Y?si=sr9ay6WbkXjx-RH2\",\n",
        "    \"https://youtu.be/pVTa7UhsyNc?si=2QFVOC-iNL_cSLRF\",\n",
        "    \"https://www.youtube.com/watch?v=PLSKmeAV43M\",\n",
        "    \"https://youtu.be/soJWJtHoplc?si=HROqo7yRTnzffEka\",\n",
        "    \"https://www.youtube.com/watch?v=PLSKmeAV43M\",\n",
        "    \"https://www.youtube.com/watch?v=QbeI72QmFAU&pp=ygUZY2F0YXJhY3Qgc3VyZ2VyeSBuYXJyYXRlZA%3D%3D\",\n",
        "    \"https://www.youtube.com/watch?v=6aIOKqBA-64\",\n",
        "    \"https://www.youtube.com/watch?v=yaAcqYn-Teo&pp=ygUZY2F0YXJhY3Qgc3VyZ2VyeSBuYXJyYXRlZA%3D%3D\",\n",
        "    \"https://www.youtube.com/watch?v=O60ZbtRcjik&pp=ygUZY2F0YXJhY3Qgc3VyZ2VyeSBuYXJyYXRlZA%3D%3D\",\n",
        "    \"https://www.youtube.com/watch?v=OZ5SmpNFlU8&pp=ygUZY2F0YXJhY3Qgc3VyZ2VyeSBuYXJyYXRlZA%3D%3D\",\n",
        "    \"https://www.youtube.com/watch?v=n_3cG9oeuNo&pp=ygUZY2F0YXJhY3Qgc3VyZ2VyeSBuYXJyYXRlZA%3D%3D\",\n",
        "    \"https://www.youtube.com/watch?v=-Q4uQ6rEExs&pp=ygUZY2F0YXJhY3Qgc3VyZ2VyeSBuYXJyYXRlZA%3D%3D\",\n",
        "    \"https://www.youtube.com/watch?v=moF1tUd9Flc&pp=ygUZY2F0YXJhY3Qgc3VyZ2VyeSBuYXJyYXRlZA%3D%3D\",\n",
        "    \"https://www.youtube.com/watch?v=aohAHNYpAOs&pp=ygUZY2F0YXJhY3Qgc3VyZ2VyeSBuYXJyYXRlZA%3D%3D\",\n",
        "    \"https://www.youtube.com/watch?v=SrzOrek2PVg\",\n",
        "    \"https://www.youtube.com/watch?v=wtw7m6C9HAw\",\n",
        "    \"https://www.youtube.com/watch?v=mfQXeuUcJdU&pp=0gcJCfsJAYcqIYzv\",\n",
        "    \"https://www.youtube.com/watch?v=s9CYeGi7ecs\",\n",
        "    \"https://www.youtube.com/watch?v=HPX8EBCVm_s\",\n",
        "    \"https://www.youtube.com/watch?v=vWCCDQgK06U&pp=0gcJCfsJAYcqIYzv\",\n",
        "    \"https://www.youtube.com/watch?v=2qEe2REdhWw\",\n",
        "    \"https://www.youtube.com/watch?v=3wwr5EzC0r4&pp=0gcJCfsJAYcqIYzv\",\n",
        "    \"https://www.youtube.com/watch?v=Q4ez4-t3WhE\",\n",
        "    \"https://www.youtube.com/watch?v=vWCCDQgK06U\",\n",
        "    \"https://www.youtube.com/watch?v=AV2ZRjYKpSA\",\n",
        "    \"https://www.youtube.com/watch?v=vfmohnIFMOQ\",\n",
        "    \"https://www.youtube.com/watch?v=2p4V1ZCQneo&pp=0gcJCfsJAYcqIYzv\",\n",
        "    \"https://www.youtube.com/watch?v=oNX7mhHewG0\",\n",
        "    \"https://www.youtube.com/watch?v=_QXWa7QaEgk\",\n",
        "    \"https://www.youtube.com/watch?v=xHdQYKq1LMs\",\n",
        "    \"https://www.youtube.com/watch?v=K2OsUADOtLc\",\n",
        "    \"https://www.youtube.com/watch?v=G4e9vrU1lrc\",\n",
        "    \"https://www.youtube.com/watch?v=Zj0hcokB5Lg&pp=0gcJCfsJAYcqIYzv\",\n",
        "    \"https://www.youtube.com/watch?v=uxXRtgQfEFI\",\n",
        "    \"https://www.youtube.com/watch?v=M8c_NoP01_A\",\n",
        "    \"https://www.youtube.com/watch?v=kD28gc6oqV4\",\n",
        "    \"https://www.youtube.com/watch?v=0xUbMicNy-w\",\n",
        "    \"https://www.youtube.com/watch?v=if2P7EPOgsY\",\n",
        "    ]\n",
        "\n",
        "    # Path to your YouTube cookies file (optional, for restricted videos)\n",
        "    cookie_file_path = 'www.youtube.com_cookies.txt'\n",
        "    if not os.path.exists(cookie_file_path):\n",
        "        print(f\"‚ö†Ô∏è Cookie file not found at '{cookie_file_path}'. Restricted content may fail.\")\n",
        "        cookie_file_path = None\n",
        "\n",
        "    print(\"\\n--- Processing Video URLs ---\")\n",
        "\n",
        "    # New: Create a list to hold all individual video URLs\n",
        "    all_individual_urls = []\n",
        "\n",
        "    # New: yt-dlp options just for info extraction to find videos in playlists\n",
        "    # 'noplaylist': False (default) is needed to process playlists.\n",
        "    info_opts = {\n",
        "        'extract_flat': 'in_playlist', # Get entries without full info\n",
        "        'skip_download': True,\n",
        "        'quiet': True,\n",
        "        'no_warnings': True, # Suppress warnings (like SABR)\n",
        "    }\n",
        "    if cookie_file_path:\n",
        "        info_opts['cookiefile'] = cookie_file_path\n",
        "\n",
        "    print(\"Inspecting provided URLs for playlists...\")\n",
        "    with yt_dlp.YoutubeDL(info_opts) as ydl:\n",
        "        for url in video_urls:\n",
        "            print(f\"Inspecting: {url}\")\n",
        "            try:\n",
        "                # Extract info\n",
        "                info = ydl.extract_info(url, download=False)\n",
        "\n",
        "                # Check if it's a playlist\n",
        "                if info.get('_type') == 'playlist':\n",
        "                    print(f\"  -> üîó Found playlist: {info.get('title', 'Unknown Playlist')}\")\n",
        "                    # Extract all video URLs from the playlist entries\n",
        "                    playlist_video_urls = [entry.get('url') for entry in info.get('entries', []) if entry and entry.get('url')]\n",
        "                    all_individual_urls.extend(playlist_video_urls)\n",
        "                    print(f\"  -> Added {len(playlist_video_urls)} videos from playlist.\")\n",
        "                else:\n",
        "                    # It's a single video, add its original URL\n",
        "                    print(\"  -> Single video found.\")\n",
        "                    all_individual_urls.append(url)\n",
        "\n",
        "            except yt_dlp.utils.DownloadError as e:\n",
        "                print(f\"  -> ‚ùå Error inspecting URL {url}: {e}\")\n",
        "            except Exception as e:\n",
        "                print(f\"  -> ‚ùå Unexpected error inspecting URL {url}: {e}\")\n",
        "\n",
        "    print(f\"\\n--- Total individual videos to process: {len(all_individual_urls)} ---\")\n",
        "\n",
        "    # Now, process each individual URL\n",
        "    for i, video_url in enumerate(all_individual_urls):\n",
        "        download_video_and_extract_audio(\n",
        "            video_url,\n",
        "            output_dir=VIDEOS_DIRECTORY,\n",
        "            audio_dir=AUDIO_DIRECTORY,\n",
        "            metadata_file=METADATA_FILE,\n",
        "            cookie_file=cookie_file_path\n",
        "        )\n",
        "\n",
        "        # Add a sleep timer after each download, except for the last one\n",
        "        if i < len(all_individual_urls) - 1:\n",
        "            print(f\"\\n--- üò¥ Sleeping for 1 seconds before next download ({i+2}/{len(all_individual_urls)}) ---\")\n",
        "            time.sleep(1)\n",
        "\n",
        "    print(\"\\nURL processing batch completed.\")\n",
        "\n",
        "    # Run verification for any videos that might have failed audio extraction\n",
        "    verify_and_process_existing_videos(VIDEOS_DIRECTORY, AUDIO_DIRECTORY)\n",
        "\n",
        "    print(\"\\n--- All processing finished ---\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVB4dIMe5N4a"
      },
      "source": [
        "### Remove Videos and Audios more than a specific threshold like 20 minutes (1200 seconds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNMCmgSA5NsJ",
        "outputId": "d9d217e9-1e52-4c54-dec3-60b81928b132"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import sys\n",
        "\n",
        "# --- Configuration ---\n",
        "# Define the maximum allowed duration in seconds (e.g., 20 minutes = 20 * 60 = 1200)\n",
        "MAX_DURATION_SECONDS = 1500\n",
        "\n",
        "# Define paths (must match your main script)\n",
        "VIDEOS_DIRECTORY = './videos'\n",
        "AUDIO_DIRECTORY = './audio'\n",
        "METADATA_FILE = os.path.join(VIDEOS_DIRECTORY, 'video_metadata.csv')\n",
        "# --- End Configuration ---\n",
        "\n",
        "\n",
        "def cleanup_long_videos(metadata_path, videos_dir, audio_dir, max_seconds):\n",
        "    \"\"\"\n",
        "    Scans a metadata CSV and removes video/audio files that exceed a\n",
        "    duration threshold. Updates the metadata file.\n",
        "    \"\"\"\n",
        "    print(\"--- Video Cleanup Utility ---\")\n",
        "\n",
        "    if not os.path.exists(metadata_path):\n",
        "        print(f\"‚ùå Error: Metadata file not found at '{metadata_path}'. Cannot proceed.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(metadata_path)\n",
        "    except pd.errors.EmptyDataError:\n",
        "        print(\"‚úÖ Metadata file is empty. Nothing to clean up.\")\n",
        "        return\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error reading metadata file: {e}\")\n",
        "        return\n",
        "\n",
        "    # Ensure 'duration_seconds' column exists\n",
        "    if 'duration_seconds' not in df.columns:\n",
        "        print(\"‚ùå Error: 'duration_seconds' column not found in metadata.\")\n",
        "        return\n",
        "\n",
        "    # Convert duration to numeric, handling errors (like 'N/A' or 'FAILED')\n",
        "    # 'coerce' will turn non-numeric values into 'NaT' (Not a Time) / 'NaN' (Not a Number)\n",
        "    df['duration_numeric'] = pd.to_numeric(df['duration_seconds'], errors='coerce')\n",
        "\n",
        "    # Find videos to keep vs. videos to remove\n",
        "    # Keep videos that are within the threshold (or have unknown duration)\n",
        "    # We use .fillna(0) so that 'NaN' values (unknown duration) are kept\n",
        "    to_keep_mask = df['duration_numeric'].fillna(0) <= max_seconds\n",
        "\n",
        "    df_to_keep = df[to_keep_mask]\n",
        "    df_to_remove = df[~to_keep_mask]\n",
        "\n",
        "    if df_to_remove.empty:\n",
        "        print(f\"‚úÖ No videos found exceeding the {max_seconds}s threshold.\")\n",
        "        # Clean up the temporary column just in case\n",
        "        if 'duration_numeric' in df.columns:\n",
        "             df = df.drop(columns=['duration_numeric'])\n",
        "             df.to_csv(metadata_path, index=False)\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(df_to_remove)} video(s) to remove (duration > {max_seconds}s):\")\n",
        "\n",
        "    # New: List videos before asking for confirmation\n",
        "    print(\"\\n--- Videos to be removed ---\")\n",
        "    for index, row in df_to_remove.iterrows():\n",
        "        title = row.get('title', f\"URL: {row.get('url', 'N/A')}\")\n",
        "        duration = row.get('duration_seconds', 'N/A')\n",
        "        print(f\"  - {title} (Duration: {duration}s)\")\n",
        "    print(\"------------------------------\")\n",
        "\n",
        "    # New: Ask for confirmation here, inside the function\n",
        "    confirm = input(\"\\nAre you sure you want to proceed with deleting these files and entries? (yes/no): \")\n",
        "    if confirm.lower() != 'yes':\n",
        "        print(\"Operation cancelled by user.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nProceeding with deletion...\")\n",
        "    removed_count = 0\n",
        "    for index, row in df_to_remove.iterrows():\n",
        "        video_name = row.get('filename')\n",
        "        audio_name = row.get('audio_filename')\n",
        "        title = row.get('title', f\"URL: {row.get('url', 'N/A')}\")\n",
        "\n",
        "        print(f\"\\nProcessing '{title}' (Duration: {row.get('duration_seconds')}s)\")\n",
        "\n",
        "        # 1. Remove Video File\n",
        "        if pd.notna(video_name) and video_name not in [\"FAILED\", \"SKIPPED_DURATION\"]:\n",
        "            video_path = os.path.join(videos_dir, video_name)\n",
        "            if os.path.exists(video_path):\n",
        "                try:\n",
        "                    os.remove(video_path)\n",
        "                    print(f\"  üóëÔ∏è Removed video: {video_path}\")\n",
        "                    removed_count += 1\n",
        "                except Exception as e:\n",
        "                    print(f\"  ‚ùå Error removing video {video_path}: {e}\")\n",
        "            else:\n",
        "                print(f\"  ü§∑ Video file not found: {video_path}\")\n",
        "        else:\n",
        "            print(f\"  ‚ÑπÔ∏è No valid video filename listed.\")\n",
        "\n",
        "        # 2. Remove Audio File\n",
        "        if pd.notna(audio_name) and audio_name not in [\"FAILED\", \"SKIPPED_DURATION\"]:\n",
        "            audio_path = os.path.join(audio_dir, audio_name)\n",
        "            if os.path.exists(audio_path):\n",
        "                try:\n",
        "                    os.remove(audio_path)\n",
        "                    print(f\"  üóëÔ∏è Removed audio: {audio_path}\")\n",
        "                    removed_count += 1\n",
        "                except Exception as e:\n",
        "                    print(f\"  ‚ùå Error removing audio {audio_path}: {e}\")\n",
        "            else:\n",
        "                print(f\"  ü§∑ Audio file not found: {audio_path}\")\n",
        "        else:\n",
        "            print(f\"  ‚ÑπÔ∏è No valid audio filename listed.\")\n",
        "\n",
        "    # 3. Update the metadata CSV file\n",
        "    try:\n",
        "        # Drop the temporary column before saving\n",
        "        df_to_keep = df_to_keep.drop(columns=['duration_numeric'])\n",
        "        df_to_keep.to_csv(metadata_path, index=False)\n",
        "        print(f\"\\n‚úÖ Successfully updated metadata file: {metadata_path}\")\n",
        "        print(f\"Removed {len(df_to_remove)} entries from CSV.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå CRITICAL: Error writing updated metadata file: {e}\")\n",
        "        print(\"   Your files may be deleted, but the CSV was not updated.\")\n",
        "\n",
        "    print(f\"\\n--- Cleanup Summary ---\")\n",
        "    print(f\"Removed {len(df_to_remove)} videos from metadata.\")\n",
        "    print(f\"Deleted {removed_count} associated files.\")\n",
        "    print(\"--- Cleanup Finished ---\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Updated main block\n",
        "    print(f\"--- Video Cleanup Utility ---\")\n",
        "    print(f\"This script will find files over {MAX_DURATION_SECONDS} seconds.\")\n",
        "    print(f\"It will read from: {METADATA_FILE}\")\n",
        "    print(f\"It will look for files in: {VIDEOS_DIRECTORY} and {AUDIO_DIRECTORY}\")\n",
        "    print(\"You will be asked for confirmation before any files are deleted.\")\n",
        "\n",
        "    # Check if a command-line argument is provided to auto-confirm\n",
        "    if len(sys.argv) > 1 and sys.argv[1].lower() == '--yes':\n",
        "        print(\"\\n'--yes' flag detected, auto-confirming...\")\n",
        "        # This part is for automation, but the main logic is now inside the function\n",
        "        # We'll just call the function, but the function itself will now ask.\n",
        "        # Let's adjust the logic. The user *probably* wants --yes to bypass the *new* check.\n",
        "\n",
        "        # Let's re-think the main block logic to better support --yes\n",
        "\n",
        "        # We need to pass the confirmation status *into* the function.\n",
        "        # I will refactor.\n",
        "        pass # Will rewrite the main block and function slightly.\n",
        "\n",
        "\n",
        "# --- Let's refactor the code to handle the confirmation logic better ---\n",
        "\n",
        "def find_videos_to_remove(metadata_path, max_seconds):\n",
        "    \"\"\"Finds videos over the duration without deleting.\"\"\"\n",
        "    if not os.path.exists(metadata_path):\n",
        "        print(f\"‚ùå Error: Metadata file not found at '{metadata_path}'. Cannot proceed.\")\n",
        "        return None, None\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(metadata_path)\n",
        "    except pd.errors.EmptyDataError:\n",
        "        print(\"‚úÖ Metadata file is empty. Nothing to clean up.\")\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error reading metadata file: {e}\")\n",
        "        return None, None\n",
        "\n",
        "    if 'duration_seconds' not in df.columns:\n",
        "        print(\"‚ùå Error: 'duration_seconds' column not found in metadata.\")\n",
        "        return None, None\n",
        "\n",
        "    df['duration_numeric'] = pd.to_numeric(df['duration_seconds'], errors='coerce')\n",
        "    to_keep_mask = df['duration_numeric'].fillna(0) <= max_seconds\n",
        "\n",
        "    df_to_keep = df[to_keep_mask]\n",
        "    df_to_remove = df[~to_keep_mask]\n",
        "\n",
        "    return df_to_keep, df_to_remove\n",
        "\n",
        "\n",
        "def delete_videos(df_to_remove, df_to_keep, metadata_path, videos_dir, audio_dir):\n",
        "    \"\"\"Performs the actual deletion of files and updates the CSV.\"\"\"\n",
        "\n",
        "    print(\"\\nProceeding with deletion...\")\n",
        "    removed_count = 0\n",
        "\n",
        "    for index, row in df_to_remove.iterrows():\n",
        "        video_name = row.get('filename')\n",
        "        audio_name = row.get('audio_filename')\n",
        "        title = row.get('title', f\"URL: {row.get('url', 'N/A')}\")\n",
        "\n",
        "        print(f\"\\nProcessing '{title}' (Duration: {row.get('duration_seconds')}s)\")\n",
        "\n",
        "        # 1. Remove Video File\n",
        "        if pd.notna(video_name) and video_name not in [\"FAILED\", \"SKIPPED_DURATION\"]:\n",
        "            video_path = os.path.join(videos_dir, video_name)\n",
        "            if os.path.exists(video_path):\n",
        "                try:\n",
        "                    os.remove(video_path)\n",
        "                    print(f\"  üóëÔ∏è Removed video: {video_path}\")\n",
        "                    removed_count += 1\n",
        "                except Exception as e:\n",
        "                    print(f\"  ‚ùå Error removing video {video_path}: {e}\")\n",
        "            else:\n",
        "                print(f\"  ü§∑ Video file not found: {video_path}\")\n",
        "        else:\n",
        "            print(f\"  ‚ÑπÔ∏è No valid video filename listed.\")\n",
        "\n",
        "        # 2. Remove Audio File\n",
        "        if pd.notna(audio_name) and audio_name not in [\"FAILED\", \"SKIPPED_DURATION\"]:\n",
        "            audio_path = os.path.join(audio_dir, audio_name)\n",
        "            if os.path.exists(audio_path):\n",
        "                try:\n",
        "                    os.remove(audio_path)\n",
        "                    print(f\"  üóëÔ∏è Removed audio: {audio_path}\")\n",
        "                    removed_count += 1\n",
        "                except Exception as e:\n",
        "                    print(f\"  ‚ùå Error removing audio {audio_path}: {e}\")\n",
        "            else:\n",
        "                print(f\"  ü§∑ Audio file not found: {audio_path}\")\n",
        "        else:\n",
        "            print(f\"  ‚ÑπÔ∏è No valid audio filename listed.\")\n",
        "\n",
        "    # 3. Update the metadata CSV file\n",
        "    try:\n",
        "        # Drop the temporary column before saving\n",
        "        if 'duration_numeric' in df_to_keep.columns:\n",
        "            df_to_keep = df_to_keep.drop(columns=['duration_numeric'])\n",
        "\n",
        "        df_to_keep.to_csv(metadata_path, index=False)\n",
        "        print(f\"\\n‚úÖ Successfully updated metadata file: {metadata_path}\")\n",
        "        print(f\"Removed {len(df_to_remove)} entries from CSV.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå CRITICAL: Error writing updated metadata file: {e}\")\n",
        "        print(\"   Your files may be deleted, but the CSV was not updated.\")\n",
        "\n",
        "    print(f\"\\n--- Cleanup Summary ---\")\n",
        "    print(f\"Removed {len(df_to_remove)} videos from metadata.\")\n",
        "    print(f\"Deleted {removed_count} associated files.\")\n",
        "    print(\"--- Cleanup Finished ---\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print(f\"--- Video Cleanup Utility ---\")\n",
        "    print(f\"This script will find files over {MAX_DURATION_SECONDS} seconds.\")\n",
        "    print(f\"It will read from: {METADATA_FILE}\")\n",
        "    print(f\"It will look for files in: {VIDEOS_DIRECTORY} and {AUDIO_DIRECTORY}\")\n",
        "\n",
        "    # 1. Find videos\n",
        "    df_to_keep, df_to_remove = find_videos_to_remove(METADATA_FILE, MAX_DURATION_SECONDS)\n",
        "\n",
        "    # 2. Check results\n",
        "    if df_to_remove is None or df_to_remove.empty:\n",
        "        if df_to_remove is not None: # This means it was empty, not an error\n",
        "             print(f\"‚úÖ No videos found exceeding the {MAX_DURATION_SECONDS}s threshold.\")\n",
        "             # We might need to save the DF to remove the temp column\n",
        "             if df_to_keep is not None and 'duration_numeric' in df_to_keep.columns:\n",
        "                 df_to_keep = df_to_keep.drop(columns=['duration_numeric'])\n",
        "                 df_to_keep.to_csv(METADATA_FILE, index=False)\n",
        "        sys.exit() # Exit script\n",
        "\n",
        "    # 3. List videos\n",
        "    print(f\"\\nFound {len(df_to_remove)} video(s) to remove (duration > {MAX_DURATION_SECONDS}s):\")\n",
        "    print(\"------------------------------\")\n",
        "    for index, row in df_to_remove.iterrows():\n",
        "        title = row.get('title', f\"URL: {row.get('url', 'N/A')}\")\n",
        "        duration = row.get('duration_seconds', 'N/A')\n",
        "        print(f\"  - {title} (Duration: {duration}s)\")\n",
        "    print(\"------------------------------\")\n",
        "\n",
        "    # 4. Check for auto-confirmation or ask user\n",
        "    auto_confirm = len(sys.argv) > 1 and sys.argv[1].lower() == '--yes'\n",
        "\n",
        "    if auto_confirm:\n",
        "        print(\"\\n'--yes' flag detected, proceeding with deletion...\")\n",
        "        proceed = True\n",
        "    else:\n",
        "        confirm = input(\"\\nAre you sure you want to proceed with deleting these files and entries? (yes/no): \")\n",
        "        proceed = confirm.lower() == 'yes'\n",
        "\n",
        "    # 5. Execute deletion if confirmed\n",
        "    if proceed:\n",
        "        delete_videos(df_to_remove, df_to_keep, METADATA_FILE, VIDEOS_DIRECTORY, AUDIO_DIRECTORY)\n",
        "    else:\n",
        "        print(\"Operation cancelled by user.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsjGuu_d1byJ",
        "outputId": "745a9e0b-42ee-4b35-b40f-033982f8ed5b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"CUDA memory cleared.\")\n",
        "else:\n",
        "    print(\"CUDA not available.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-h7V_Cb2Jom",
        "outputId": "d1caa6e5-038c-41fa-ea5f-536b9cd0d69b"
      },
      "outputs": [],
      "source": [
        "!pip install -q git+https://github.com/openai/whisper.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcLT3SUT2OOA",
        "outputId": "e24edc14-213b-4c1c-91e8-5334fba9d349"
      },
      "outputs": [],
      "source": [
        "# pip install git+https://github.com/openai/whisper.git\n",
        "\n",
        "import os\n",
        "import whisper\n",
        "import torch\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "def transcribe_audio_files(input_dir: str = './audio', output_dir: str = './transcripts'):\n",
        "    \"\"\"\n",
        "    Transcribes all .wav files in the input directory using Whisper's large-v3 model,\n",
        "    capturing sentence-level timestamps, and saves the output as .json files.\n",
        "\n",
        "      Args:\n",
        "        input_dir: The directory containing the .wav files (16kHz mono).\n",
        "        output_dir: The directory where the transcription .json files will be saved.\n",
        "    \"\"\"\n",
        "    print(\"--- Starting Audio Transcription Process ---\")\n",
        "\n",
        "    # 1. Setup directories and check for GPU\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f\"Using device: {device.upper()}\")\n",
        "    if device == 'cpu':\n",
        "        print(\"‚ö†Ô∏è WARNING: No GPU found. Transcription will be very slow.\")\n",
        "\n",
        "    # 2. Load the pre-trained Whisper model\n",
        "    print(\"Loading Whisper model (large)...\")\n",
        "    try:\n",
        "        model = whisper.load_model(\"large\", device=device)\n",
        "        print(\"‚úÖ Model loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading Whisper model: {e}\")\n",
        "        print(\"Please check your internet connection and if 'openai-whisper' is installed correctly.\")\n",
        "        return\n",
        "\n",
        "    # 3. Identify audio files to process\n",
        "    audio_files = {os.path.splitext(f)[0] for f in os.listdir(input_dir) if f.endswith('.wav')}\n",
        "    transcribed_files = {os.path.splitext(f)[0] for f in os.listdir(output_dir) if f.endswith('.json')}\n",
        "    files_to_process = sorted([f + '.wav' for f in (audio_files - transcribed_files)])\n",
        "\n",
        "    if not files_to_process:\n",
        "        print(\"‚úÖ All audio files have already been transcribed.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(files_to_process)} audio file(s) to transcribe.\")\n",
        "\n",
        "    # 4. Process each audio file\n",
        "    for filename in tqdm(files_to_process, desc=\"Transcribing Audio\"):\n",
        "        input_path = os.path.join(input_dir, filename)\n",
        "        output_filename = f\"{os.path.splitext(filename)[0]}.json\"\n",
        "        output_path = os.path.join(output_dir, output_filename)\n",
        "\n",
        "        try:\n",
        "            # Perform transcription (sentence/segment-level timestamps by default)\n",
        "            result = model.transcribe(input_path, fp16=torch.cuda.is_available())\n",
        "\n",
        "            # Extract sentence-level segments and round timestamps to one decimal place\n",
        "            segments = [\n",
        "                {\n",
        "                    \"start\": round(seg[\"start\"], 1),\n",
        "                    \"end\": round(seg[\"end\"], 1),\n",
        "                    \"text\": seg[\"text\"].strip()\n",
        "                }\n",
        "                for seg in result[\"segments\"]\n",
        "            ]\n",
        "\n",
        "            # Save just the clean sentence-level transcript with timestamps\n",
        "            with open(output_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(segments, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "        except Exception as e:\n",
        "            tqdm.write(f\"‚ùå Error transcribing {filename}: {e}\")\n",
        "\n",
        "    print(\"\\n--- Audio Transcription process completed. ---\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Make sure ffmpeg is installed on your system and accessible in your PATH.\n",
        "    # On Debian /Ubuntu: sudo apt update && sudo apt install ffmpeg\n",
        "    # On macOS (using Homebrew): brew install ffmpeg\n",
        "    # On Windows (using Chocolatey): choco install ffmpeg\n",
        "    transcribe_audio_files()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKbLZLiOKykx",
        "outputId": "2bf744c4-f155-4d86-88be-f01794dbe28c"
      },
      "outputs": [],
      "source": [
        "!pip -q install cerebras-cloud-sdk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTnnINX649fh",
        "outputId": "d3e866c4-3c2b-4e0f-ed3b-96251fff5f1a"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Transcript Refinement Pipeline (to TXT) ‚Äì FINAL CLEAN VERSION\n",
        "No cv2 ‚Üí No RAM crash | No duration ‚Üí Faster & lighter | Video matching preserved\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import csv\n",
        "\n",
        "# --- Colab or Local ---\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    COLAB_ENV = True\n",
        "except ImportError:\n",
        "    from dotenv import load_dotenv\n",
        "    COLAB_ENV = False\n",
        "\n",
        "from cerebras.cloud.sdk import Cerebras\n",
        "\n",
        "\n",
        "# --- Configuration ---\n",
        "INPUT_DIR = './transcripts'\n",
        "VIDEOS_DIR = './videos'\n",
        "REFINED_OUTPUT_DIR = './refined_transcripts'\n",
        "FULL_RESPONSE_DIR = os.path.join(REFINED_OUTPUT_DIR, 'full_responses')\n",
        "CSV_LOG_PATH = os.path.join(REFINED_OUTPUT_DIR, 'refinement_log.csv')\n",
        "\n",
        "API_CALL_DELAY_SECONDS = 20\n",
        "MAX_FILES_PER_RUN = 300\n",
        "\n",
        "# MODEL_NAME = \"qwen-3-235b-a22b-thinking-2507\"\n",
        "MODEL_NAME = \"qwen-3-235b-a22b-instruct-2507\"\n",
        "\n",
        "MEDICAL_EDITOR_SYSTEM_PROMPT = \"\"\"You are an expert JSON and medical editor. Your task is to correct typos, punctuation, and grammatical errors in a JSON file provided by the user, while preserving its exact structure.\n",
        "\n",
        "The user will provide a JSON array of segments from a cataract surgery video.\n",
        "Your job is to fix errors **only** in the \"text\" fields.\n",
        "\n",
        "**CRITICAL INSTRUCTIONS:**\n",
        "1.  Read the user's JSON, perform your corrections, and think.\n",
        "2.  You **MUST** return the JSON in the **EXACT** same array format, including \"start\", \"end\", and \"text\" keys for every segment.\n",
        "3.  **DO NOT** alter the \"start\", \"end\", or any other part of the JSON structure.\n",
        "4.  **DO NOT** include any commentary, conversational replies, or pre-amble.\n",
        "5.  The output must be the pure, corrected JSON data and nothing else.\"\"\"\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Helper Functions\n",
        "# ==============================================================================\n",
        "\n",
        "def get_api_key():\n",
        "    if COLAB_ENV:\n",
        "        return userdata.get('CEREBRAS_API_KEY')\n",
        "    else:\n",
        "        return os.environ.get(\"CEREBRAS_API_KEY\")\n",
        "\n",
        "def parse_llm_json_output(raw_output: str) -> str:\n",
        "    \"\"\"\n",
        "    Extracts JSON array from a model output.\n",
        "    Works for:\n",
        "      ‚Ä¢ Thinking models ‚Üí have <think> ... </think> tags\n",
        "      ‚Ä¢ Instruct models ‚Üí no think tag at all\n",
        "\n",
        "    Always returns ONLY the JSON array [ ... ].\n",
        "    \"\"\"\n",
        "\n",
        "    THINK_END_TAG = \"</think>\"\n",
        "\n",
        "    # --- Case 1: Thinking model output ---\n",
        "    if THINK_END_TAG in raw_output:\n",
        "        after_think = raw_output.split(THINK_END_TAG, 1)[1].strip()\n",
        "    else:\n",
        "        # --- Case 2: Instruct model output (no think tag) ---\n",
        "        after_think = raw_output.strip()\n",
        "\n",
        "    # Extract the JSON array between the first `[` and last `]`\n",
        "    start = after_think.find(\"[\")\n",
        "    end = after_think.rfind(\"]\")\n",
        "\n",
        "    if start != -1 and end != -1 and end > start:\n",
        "        return after_think[start:end + 1].strip()\n",
        "\n",
        "    # If for some reason JSON is malformed, return everything\n",
        "    return after_think\n",
        "\n",
        "\n",
        "def format_time_to_mm_ss(seconds: float) -> str:\n",
        "    s = int(seconds)\n",
        "    return f\"{s//60:02d}:{s%60:02d}\"\n",
        "\n",
        "def format_segments_to_txt(segments: list) -> str:\n",
        "    lines = []\n",
        "    for seg in segments:\n",
        "        try:\n",
        "            start = format_time_to_mm_ss(seg[\"start\"])\n",
        "            end = format_time_to_mm_ss(seg[\"end\"])\n",
        "            text = seg[\"text\"].strip()\n",
        "            lines.append(f\"[{start} - {end}]: {text}\")\n",
        "        except KeyError:\n",
        "            continue\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def find_matching_video(videos_dir: str, basename: str) -> tuple[str, bool]:\n",
        "    \"\"\"Returns (video_filename_or_NOT_FOUND, found_bool)\"\"\"\n",
        "    if not os.path.isdir(videos_dir):\n",
        "        return \"NOT_FOUND\", False\n",
        "\n",
        "    target = basename.lower()\n",
        "    for filename in os.listdir(videos_dir):\n",
        "        name, ext = os.path.splitext(filename)\n",
        "        if name.lower() == target and ext.lower() in {'.mp4','.mov','.avi','.mkv','.webm','.flv','.m4v','.wmv','.mpeg','.mpg'}:\n",
        "            return filename, True\n",
        "    return \"NOT_FOUND\", False\n",
        "\n",
        "def refine_with_llm(text: str, model_name: str, api_key: str) -> str | None:\n",
        "    try:\n",
        "        client = Cerebras(api_key=api_key)\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": MEDICAL_EDITOR_SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": text}\n",
        "        ]\n",
        "        response = client.chat.completions.create(messages=messages, model=model_name)\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        if any(x in str(e).lower() for x in [\"context\", \"limit\", \"too large\"]):\n",
        "            tqdm.write(f\"Context limit exceeded: {e}\")\n",
        "            return '{\"error\": \"MODEL CONTEXT LIMIT EXCEEDED\"}'\n",
        "        tqdm.write(f\"API error: {e} ‚Üí retrying in 10s...\")\n",
        "        time.sleep(10)\n",
        "        return None\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# CSV Logging\n",
        "# ==============================================================================\n",
        "\n",
        "def setup_csv_log():\n",
        "    if os.path.exists(CSV_LOG_PATH):\n",
        "        return\n",
        "    with open(CSV_LOG_PATH, 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\n",
        "            \"video_name\",\n",
        "            \"transcript_name\",\n",
        "            \"total_characters\",\n",
        "            \"total_words\",\n",
        "            \"video_found\",\n",
        "            \"success\"\n",
        "        ])\n",
        "\n",
        "def log_result(row: list):\n",
        "    with open(CSV_LOG_PATH, 'a', newline='', encoding='utf-8') as f:\n",
        "        csv.writer(f).writerow(row)\n",
        "\n",
        "# ==============================================================================\n",
        "# Main Pipeline\n",
        "# ==============================================================================\n",
        "\n",
        "def refine_transcripts():\n",
        "    print(\"\\n=== TRANSCRIPT REFINEMENT PIPELINE (LIGHT & STABLE) ===\\n\")\n",
        "\n",
        "    os.makedirs(REFINED_OUTPUT_DIR, exist_ok=True)\n",
        "    os.makedirs(FULL_RESPONSE_DIR, exist_ok=True)\n",
        "    setup_csv_log()\n",
        "\n",
        "    api_key = get_api_key()\n",
        "    if not api_key:\n",
        "        print(\"CEREBRAS_API_KEY not found!\")\n",
        "        return\n",
        "\n",
        "    # Find files to process\n",
        "    try:\n",
        "        raw = {os.path.splitext(f)[0] for f in os.listdir(INPUT_DIR) if f.endswith('.json')}\n",
        "        done = {os.path.splitext(f)[0] for f in os.listdir(REFINED_OUTPUT_DIR) if f.endswith('.txt')}\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Directory not found: {e}\")\n",
        "        return\n",
        "\n",
        "    to_process = sorted(raw - done)[:MAX_FILES_PER_RUN]\n",
        "    if not to_process:\n",
        "        print(\"All transcripts already refined!\")\n",
        "        return\n",
        "\n",
        "    print(f\"Processing {len(to_process)} transcript(s)...\\n\")\n",
        "\n",
        "    for basename in tqdm(to_process, desc=\"Refining\"):\n",
        "        video_name, video_found = find_matching_video(VIDEOS_DIR, basename)\n",
        "        if video_found:\n",
        "            tqdm.write(f\"  Found video: {video_name}\")\n",
        "        else:\n",
        "            tqdm.write(f\"  No video found for: {basename}\")\n",
        "\n",
        "        json_path = os.path.join(INPUT_DIR, f\"{basename}.json\")\n",
        "        txt_path = os.path.join(REFINED_OUTPUT_DIR, f\"{basename}.txt\")\n",
        "        full_path = os.path.join(FULL_RESPONSE_DIR, f\"{basename}_full_response.txt\")\n",
        "\n",
        "        # Load original\n",
        "        try:\n",
        "            with open(json_path, 'r', encoding='utf-8') as f:\n",
        "                segments = json.load(f)\n",
        "        except Exception as e:\n",
        "            tqdm.write(f\"Failed to load {basename}.json ‚Üí {e}\")\n",
        "            continue\n",
        "\n",
        "        if not segments:\n",
        "            tqdm.write(f\"Empty transcript: {basename}\")\n",
        "            continue\n",
        "\n",
        "        payload = json.dumps(segments, indent=4)\n",
        "\n",
        "        # Call LLM\n",
        "        raw_response = None\n",
        "        while raw_response is None:\n",
        "            raw_response = refine_with_llm(payload, MODEL_NAME, api_key)\n",
        "\n",
        "        # Save raw response\n",
        "        with open(full_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(raw_response)\n",
        "\n",
        "        # Parse and save refined .txt\n",
        "        success = False\n",
        "        total_chars = total_words = 0\n",
        "        try:\n",
        "            cleaned_json = parse_llm_json_output(raw_response)\n",
        "            refined_segments = json.loads(cleaned_json)\n",
        "\n",
        "            total_words = sum(len(seg.get(\"text\", \"\").split()) for seg in refined_segments)\n",
        "            formatted_text = format_segments_to_txt(refined_segments)\n",
        "            total_chars = len(formatted_text)\n",
        "\n",
        "            with open(txt_path, 'w', encoding='utf-8') as f:\n",
        "                f.write(formatted_text)\n",
        "\n",
        "            success = True\n",
        "        except Exception as e:\n",
        "            tqdm.write(f\"Failed to parse/save {basename} ‚Üí {e}\")\n",
        "\n",
        "        # Log result\n",
        "        log_result([\n",
        "            video_name,\n",
        "            f\"{basename}.txt\",\n",
        "            total_chars,\n",
        "            total_words,\n",
        "            video_found,\n",
        "            success\n",
        "        ])\n",
        "\n",
        "        # Rate limit\n",
        "        if basename != to_process[-1]:\n",
        "            time.sleep(API_CALL_DELAY_SECONDS)\n",
        "\n",
        "    print(\"\\nALL DONE ‚Äì NO MORE COLAB CRASHES!\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Run\n",
        "# ==============================================================================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    if not COLAB_ENV:\n",
        "        load_dotenv()\n",
        "    refine_transcripts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QP-duHEdtNfr",
        "outputId": "d0f11738-4b00-4c10-889d-a5792471b81a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "videos_list = os.listdir(\"./videos\")\n",
        "print(f\"Number of Videos {len(videos_list)}\")\n",
        "print(f\"Number of Audios {len(os.listdir('./audio'))}\")\n",
        "print(f\"Number of transcripts {len(os.listdir('./transcripts'))}\")\n",
        "print(f\"Number of refined transcripts: {len(os.listdir('./refined_transcripts/'))}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
